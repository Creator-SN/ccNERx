{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.预测模型 3.扩展数据集(ccks+FN) 4.扩展数据集用于预训练(ccks+FN) 5.验证(四川的500条上) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.训练出ccks的预测模型\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/ccks/train-json.csv',\n",
    "    # 'eval_file': './data/ccks/dev.csv',\n",
    "    # 'test_file': './data/ccks/test.csv',\n",
    "    'eval_file': './data/FN/sc-json/dev.csv',\n",
    "    'test_file': './data/FN/sc-json/test.csv',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'output_eval': True,\n",
    "    'loader_name': 'le_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'ccks_predict_model'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.预测四川数据集（多标签）\n",
    "from CC.predicter import NERPredict\n",
    "import json\n",
    "\n",
    "# 使用了预训练模型\n",
    "args[\"lstm_crf_model_file\"] = \"save_model/ccks_predict_model/lstm_crf/lstm_crf_1320.pth\"\n",
    "args[\"bert_model_file\"] = \"save_model/ccks_predict_model/LEBert/LEBert_1320.pth\"\n",
    "predict = NERPredict(**args)\n",
    "\n",
    "filename = \"data/FN/sc-super/sc-train_400.json\"\n",
    "\n",
    "batch_size = 64\n",
    "index = 0\n",
    "sentences = []\n",
    "\n",
    "with open(\"data/FN/sc-super/sc_super_400.json\", \"w\", encoding=\"utf-8\") as out:\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            text = data[\"text\"]\n",
    "            \n",
    "            sentences.append(text)\n",
    "            index += 1\n",
    "            if index % batch_size == batch_size-1:\n",
    "                for s, label in predict(sentences):\n",
    "                    \n",
    "                    assert len(s[:args[\"max_seq_length\"]-2])==len(label)\n",
    "                    out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")\n",
    "                sentences = []\n",
    "                out.flush()\n",
    "        if len(sentences)>0:\n",
    "            for s, label in predict(sentences):\n",
    "                assert len(s[:args[\"max_seq_length\"]])==len(label)\n",
    "                out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert预训练\n",
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/FN_multiple_pretrained_2/Bert_8960/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/sc-super/train_super.json',\n",
    "    'eval_file': './data/FN/sc-json-500/dev.json',\n",
    "    'test_file': './data/FN/sc-json-500/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks+FN_tags_list.txt',\n",
    "    'loader_name': 'ptloader_v2',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'FN_multiple_pretrained_3',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"KEYWORD\": \"关键词\",\n",
    "        \"DIS\": \"疾病或诊断\",\n",
    "        \"ANA\": \"解剖部位\",\n",
    "        \"LAB\": \"实验室检验\",\n",
    "        \"MED\": \"药物\",\n",
    "        \"OPE\": \"手术\",\n",
    "        \"IMA\": \"影像检查\",\n",
    "    }\n",
    "}\n",
    "\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer(lr=1e-5):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.验证2\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/FN_multiple_pretrained_3/Bert_33600/pytorch_model.bin',\n",
    "\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/ccks/train.json',\n",
    "    # 'eval_file': './data/ccks/sc-json-500/dev.json',\n",
    "    # 'test_file': './data/ccks/sc-json-500/test.json',\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/sc-json-500/dev.json',\n",
    "    'test_file': './data/FN/sc-json-500/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 4,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_dev_3'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/sc-json/dev.csv',\n",
    "    'test_file': './data/FN/sc-json/test.csv',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 4,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_dev_base_2'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1预训练(ccks+FJ)   2.预测模型 3.扩展数据集(ccks+FN) 4.扩展数据集用于预训练(ccks+FN) 5.验证(四川的500条上) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.预训练\n",
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/ccks_pretrained/Bert_3160/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/ccks/ccks/train_all.json',\n",
    "    'eval_file': './data/ccks/dev.json',\n",
    "    'test_file': './data/ccks/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'loader_name': 'ptloader_v2',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'ccks_pretrained_2',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"KEYWORD\":\"异常关键词\",\n",
    "        \"DIS\": \"疾病或诊断\",\n",
    "        \"ANA\": \"解剖部位\",\n",
    "        \"LAB\": \"实验室检验\",\n",
    "        \"MED\": \"药物\",\n",
    "        \"OPE\": \"手术\",\n",
    "        \"IMA\": \"影像检查\",\n",
    "    }\n",
    "}\n",
    "\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.预测 修改：训练集要替换成ccks，预测ccks标签+keyword\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/ccks_pretrained/Bert_6055/pytorch_model.bin', #第一次的 只有ccks的预训练\n",
    "    # 'pretrained_file_name': './save_pretrained/ccks_pretrained/Bert_6055/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/ccks/ccks+FN_FJ/train.json',\n",
    "    'eval_file': './data/FN/sc-json-500/dev.json',\n",
    "    'test_file': './data/FN/sc-json-500/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks+FN_tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 32,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN+CCKS_predict_model'\n",
    "}\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer():\n",
    "#     a = i\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 扩展数据集   修改：要扩展ccks标签+FN\n",
    "from CC.predicter import NERPredict\n",
    "import json\n",
    "\n",
    "# 使用了预训练模型\n",
    "args[\"lstm_crf_model_file\"] = \"save_model/FN+CCKS_predict_model/lstm_crf/lstm_crf_.pth\"\n",
    "args[\"bert_model_file\"] = \"save_model/FN+CCKS_predict_model/LEBert/LEBert_.pth\"\n",
    "predict = NERPredict(**args)\n",
    "\n",
    "filename = \"data/FN/fj+sc/train(400).json\"\n",
    "\n",
    "batch_size = 40\n",
    "index = 0\n",
    "sentences = []\n",
    "\n",
    "with open(\"data/FN/sc_fj_tags_keyword/train(400)_ccks.json\", \"w\", encoding=\"utf-8\") as out:\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            text = data[\"text\"]\n",
    "            \n",
    "            sentences.append(text)\n",
    "            index += 1\n",
    "            if index % batch_size == batch_size-1:\n",
    "                for s, label in predict(sentences):\n",
    "                    \n",
    "                    assert len(s[:args[\"max_seq_length\"]-2])==len(label)\n",
    "                    out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")\n",
    "                sentences = []\n",
    "                out.flush()\n",
    "        if len(sentences)>0:\n",
    "            for s, label in predict(sentences):\n",
    "                assert len(s[:args[\"max_seq_length\"]])==len(label)\n",
    "                out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练 \n",
    "# sc&fj_tags_ccks_pretrain:多标签的sc和fj数据集（各400条）进行预训练\n",
    "# sc&fj_tags_ccks_pretrain_2:跟上面一样 lr=1e-5\n",
    "# sc_fj_tags_keyword_pretrain:keyword标签的sc和fj数据集（各400条）进行预训练\n",
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc&fj_tags_ccks_pretrain/Bert_11640/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/sc_fj_tags_keyword/fj_400_sc_400.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.jsonn',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    # 'tag_file': './data/ccks/ccks+FN_tags_list.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'ptloader_v2',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'sc_fj_tags_keyword_pretrain',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"KEYWORD\":\"异常关键词\",\n",
    "        \"DIS\": \"疾病或诊断\",\n",
    "        \"ANA\": \"解剖部位\",\n",
    "        \"LAB\": \"实验室检验\",\n",
    "        \"MED\": \"药物\",\n",
    "        \"OPE\": \"手术\",\n",
    "        \"IMA\": \"影像检查\",\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer(lr=1e-4):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/sc&fj_tags_ccks_pretrain_2/Bert_10088/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    # 'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_fj_sc_tag_ccks_pre_2'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.外部数据集训练一个预测模型 2.把福建和四川总共800条的数据集标签拓展成多标签的 \n",
    "#### 3.把数据集预训练 4.验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.预测 \n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/ccks_pretrained_2/Bert_6055/pytorch_model.bin', \n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/ccks/train.json',\n",
    "    'eval_file': './data/ccks/dev.json',\n",
    "    'test_file': './data/ccks/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks+FN_tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'ccks_predict_model_2'\n",
    "}\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer():\n",
    "#     a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.预测fj+sc数据集（多标签）\n",
    "from CC.predicter import NERPredict\n",
    "import json\n",
    "\n",
    "# 使用了预训练模型\n",
    "args[\"lstm_crf_model_file\"] = \"save_model/ccks_predict_model_2/lstm_crf/lstm_crf_690.pth\"\n",
    "args[\"bert_model_file\"] = \"save_model/ccks_predict_model_2/LEBert/LEBert_690.pth\"\n",
    "\n",
    "\n",
    "predict = NERPredict(**args)\n",
    "\n",
    "filename = \"data/FN/sc_fj_tags_keyword/fj_400_sc_400.json\"\n",
    "\n",
    "batch_size = 64\n",
    "index = 0\n",
    "sentences = []\n",
    "\n",
    "with open(\"data/FN/sc_fj_tags_ccks/fj_400_sc_400_ccks.json\", \"w\", encoding=\"utf-8\") as out:\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            text = data[\"text\"]\n",
    "            \n",
    "            sentences.append(text)\n",
    "            index += 1\n",
    "            if index % batch_size == batch_size-1:\n",
    "                for s, label in predict(sentences):\n",
    "                    \n",
    "                    assert len(s[:args[\"max_seq_length\"]-2])==len(label)\n",
    "                    out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")\n",
    "                sentences = []\n",
    "                out.flush()\n",
    "        if len(sentences)>0:\n",
    "            for s, label in predict(sentences):\n",
    "                assert len(s[:args[\"max_seq_length\"]])==len(label)\n",
    "                out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练 \n",
    "# sc&fj_tags_ccks_pretrain:多标签的sc和fj数据集（各400条）进行预训练  需要重新预训练\n",
    "# sc&fj_tags_ccks_pretrain_2:跟上面一样 lr=1e-5\n",
    "# sc_fj_tags_keyword_pretrain:keyword标签的sc和fj数据集（各400条）进行预训练\n",
    "\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='2, 3'\n",
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/sc_fj_tags_keyword_pretrain/Bert_12080/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/sc_fj_tags_keyword/fj_400_sc_400.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.jsonn',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    # 'tag_file': './data/ccks/ccks+FN_tags_list.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'ptloader_v2',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'sc_fj_tags_keyword_pretrain_2',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"KEYWORD\":\"异常关键词\",\n",
    "        \"DIS\": \"疾病或诊断\",\n",
    "        \"ANA\": \"解剖部位\",\n",
    "        \"LAB\": \"实验室检验\",\n",
    "        \"MED\": \"药物\",\n",
    "        \"OPE\": \"手术\",\n",
    "        \"IMA\": \"影像检查\",\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer(lr=1e-5):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 lr2=1e-2\n",
    "#  5.验证 7是有左括号切分 8是没有左括号 9是强制120切分 10在9的基础上120匹配左括号切分 67% 11 keyword 12 lexloader \n",
    "#  只有400条的sc数据集做多标签预训练  13 训练集只有四川   14 800条 15 只有keyword \n",
    "#  16 只有福建，训练测试验证也是福建。出现了0\n",
    "#  17 在训练集只有福建\n",
    "#  FN_fj_sc_tag_ccks_pre_2_eval ccks多标签 测试集100\n",
    "#  FN_sc_fj_tags_keyword_pretrain_2 只有keyword\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/sc_fj_tags_keyword_pretrain_2/Bert_15100/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    # 'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_sc_fj_tags_keyword_pretrain_2'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 lr2=1e-2\n",
    "#  5.验证 7是有左括号切分 8是没有左括号 9是强制120切分 10在9的基础上120匹配左括号切分 67% 11 keyword 12 lexloader \n",
    "#  只有400条的sc数据集做多标签预训练  13 训练集只有四川   14 800条 15 只有keyword\n",
    "# \n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/sc&fj_tags_ccks_pretrain_2/Bert_10088/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    # 'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_fj_sc_tag_ccks_pre_2'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 转换成json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.to_json import conll_to_json\n",
    "conll_to_json('./data/ccks/subtask1_training_part2.txt', './data/ccks/3.csv', split_tag='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.to_json import conll_to_json\n",
    "conll_to_json('./data/FN/sc-super/dev.csv', './data/FN/sc-super/dev.json', split_tag='\\n\\n')\n",
    "conll_to_json('./data/FN/sc-super/test.csv', './data/FN/sc-super/test.json', split_tag='\\n\\n')\n",
    "conll_to_json('./data/FN/sc-super/train.csv', './data/FN/sc-super/train.json', split_tag='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = \"data/FN/fj+sc/train(400).json\"\n",
    "sum = 0\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        text = data[\"text\"]\n",
    "        label = data[\"label\"]\n",
    "        \n",
    "        if len(text)>2000:\n",
    "            sum = sum+1\n",
    "print(sum)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "from tqdm import *\n",
    "from CC.loaders.pretrain import *\n",
    "from CC.loaders.pretrain.ptloader_v2 import *\n",
    "import json\n",
    "from CC.predicter import NERPredict\n",
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/FN_multiple_pretrained_400_6/Bert_1200/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/sc-super/train_super_400.json',\n",
    "    'eval_file': './data/FN/sc-json-500/dev.json',\n",
    "    'test_file': './data/FN/sc-json-500/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks+FN_tags_list.txt',\n",
    "    'loader_name': 'ptloader_v2',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'FN_multiple_pretrained_400_6',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"KEYWORD\":\"关键词\",\n",
    "        \"DIS\": \"疾病诊断\",\n",
    "        \"ANA\": \"解剖部位\",\n",
    "        \"LAB\": \"实验检验\",\n",
    "        \"MED\": \"药物\",\n",
    "        \"OPE\": \"手术\",\n",
    "        \"IMA\": \"影像检查\",\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "loader = PTLoaderV2(**args)\n",
    "\n",
    "\n",
    "# loader.tag_vocab.id2token(loader.myData[0][\"labels\"].tolist())\n",
    "\n",
    "# loader.tokenizer.decode(loader.myData[0][\"input_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.myData[799][\"input_ids\"]\n",
    "loader.tokenizer.decode(loader.myData[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.myData[0][\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 重新验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/FN/FN_spuer_pretrained_400_5/Bert_12832/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    # 'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_sc_test100_pre'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN/FN_spuer_pretrained_400_5/Bert_12832/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    # 'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 4,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_sc_test100_bs_4'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/FN_spuer_pretrained_fj_pt/Bert_21700/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    # 'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'eval_file': './data/FN/fj+sc/test(100+100).json',\n",
    "    'test_file': './data/FN/fj+sc/test(100+100).json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'FN_sc_fj_test200'\n",
    "}\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 验证仅四川的和福建的（ccks标签&keyword标签）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.预测 \n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/ccks_pretrained_2/Bert_6055/pytorch_model.bin', \n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/ccks/train.json',\n",
    "    'eval_file': './data/ccks/dev.json',\n",
    "    'test_file': './data/ccks/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks+FN_tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'ccks_predict_model_2'\n",
    "}\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer():\n",
    "#     a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拓展数据集\n",
    "from CC.predicter import NERPredict\n",
    "import json\n",
    "\n",
    "# 使用了预训练模型\n",
    "args[\"lstm_crf_model_file\"] = \"save_model/ccks_predict_model_2/lstm_crf/lstm_crf_690.pth\"\n",
    "args[\"bert_model_file\"] = \"save_model/ccks_predict_model_2/LEBert/LEBert_690.pth\"\n",
    "predict = NERPredict(**args)\n",
    "\n",
    "filename = \"data/FN/sc_json_500/pretrain_keyword.json\"\n",
    "\n",
    "batch_size = 40\n",
    "index = 0\n",
    "sentences = []\n",
    "\n",
    "with open(\"data/FN/sc_json_500/pretrain_ccks.json\", \"w\", encoding=\"utf-8\") as out:\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            text = data[\"text\"]\n",
    "            \n",
    "            sentences.append(text)\n",
    "            index += 1\n",
    "            if index % batch_size == batch_size-1:\n",
    "                for s, label in predict(sentences):\n",
    "                    \n",
    "                    assert len(s[:args[\"max_seq_length\"]-2])==len(label)\n",
    "                    out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")\n",
    "                sentences = []\n",
    "                out.flush()\n",
    "        if len(sentences)>0:\n",
    "            for s, label in predict(sentences):\n",
    "                assert len(s[:args[\"max_seq_length\"]])==len(label)\n",
    "                out.write(f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练\n",
    "# sc_ccks_pretrain_3:sc的ccks预训练 13600\n",
    "# sc_keyword_pretrain_2:sc的keyword预训练 34000\n",
    "# fj_keyword_pretrain_2 27200\n",
    "# fj_ccks_pretrain_2 10940\n",
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/fj_ccks_pretrain_1/Bert_4080/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/FN/fj_json/pretrain_keyword.json',\n",
    "    'eval_file': './data/FN/fj_json/eval.json',\n",
    "    'test_file': './data/FN/fj_json/test.jsonn',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/ccks/ccks+FN_tags_list.txt',\n",
    "    # 'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'ptloader_v2',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'fj_ccks_pretrain_2',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"KEYWORD\":\"异常关键词\",\n",
    "        \"DIS\": \"疾病或诊断\",\n",
    "        \"ANA\": \"解剖部位\",\n",
    "        \"LAB\": \"实验室检验\",\n",
    "        \"MED\": \"药物\",\n",
    "        \"OPE\": \"手术\",\n",
    "        \"IMA\": \"影像检查\",\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer(lr=1e-5):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/sc_json_500/train.json',\n",
    "    'eval_file': './data/FN/sc_test100/test.json',\n",
    "    'test_file': './data/FN/sc_test100/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/FN/medicine_vocab.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    # 'task_name': 'sc_medicine_vocab_baseline_4'\n",
    "}\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/FN/medicine_vocab.txt\",\n",
      "    \"train_file\": \"./data/FN/sc_json_500/train.json\",\n",
      "    \"eval_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"test_file\": \"./data/FN/sc_test100/test.json\",\n",
      "    \"tag_file\": \"./data/FN/tags_list.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"sc_medicine_pretrain_ccks_2\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/FN/sc_json_500/train.json etag: 100%|██████████| 4.71M/4.71M [00:00<00:00, 364MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 371MB/s]\n",
      "calculate ./data/FN/sc_test100/test.json etag: 100%|██████████| 1.18M/1.18M [00:00<00:00, 364MB/s]\n",
      "calculate ./data/FN/tags_list.txt etag: 100%|██████████| 21.0/21.0 [00:00<00:00, 62.7kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/matched_words\n",
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/word_vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/FN/tags_list.txt: 3L [00:00, 21112.27L/s]\n",
      "build line mapper: 3L [00:00, 5361.27L/s]/3 [00:00<?, ?it/s]\n",
      "load vocab from files: 100%|██████████| 3/3 [00:00<00:00, 893.36it/s]\n",
      "load vocab from list: 100%|██████████| 3/3 [00:00<00:00, 47662.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/88ff756ec83be36f241527dc9bf49e5d_2bee60a32e19544d149be8c360d1a109_2bee60a32e19544d149be8c360d1a109_b43c9a3dc281dcbcfc242873c520c19f/1000000/vocab_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load dataset from ./data/FN/sc_json_500/train.json: 100%|██████████| 400/400 [00:02<00:00, 153.36it/s]\n",
      "load dataset from ./data/FN/sc_test100/test.json: 100%|██████████| 100/100 [00:00<00:00, 150.34it/s]\n",
      "Some weights of the model checkpoint at ./save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin were not used when initializing LEBertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing LEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LEBertModel were not initialized from the model checkpoint at ./save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.fuse_layernorm.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.word_transform.bias', 'word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.20it/s, F1=0.001, train_acc=0.79, train_loss=227, train_precision=0.02, train_recall=0.000513]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.00301, eval_acc=0.912, eval_loss=97.4, eval_precision=0.0278, eval_recall=0.00159]\n",
      "Epoch: 2/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.0528, train_acc=0.916, train_loss=85.7, train_precision=0.118, train_recall=0.0389]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.161, eval_acc=0.921, eval_loss=65.4, eval_precision=0.18, eval_recall=0.146] \n",
      "Epoch: 3/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.266, train_acc=0.931, train_loss=57.2, train_precision=0.345, train_recall=0.225] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.486, eval_acc=0.94, eval_loss=45.3, eval_precision=0.468, eval_recall=0.505] \n",
      "Epoch: 4/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.20it/s, F1=0.487, train_acc=0.942, train_loss=39.9, train_precision=0.53, train_recall=0.458] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.515, eval_acc=0.929, eval_loss=39.6, eval_precision=0.457, eval_recall=0.592]\n",
      "Epoch: 5/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.547, train_acc=0.948, train_loss=30.3, train_precision=0.592, train_recall=0.523]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.541, eval_acc=0.94, eval_loss=30.2, eval_precision=0.498, eval_recall=0.592] \n",
      "Epoch: 6/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.20it/s, F1=0.607, train_acc=0.956, train_loss=22.6, train_precision=0.632, train_recall=0.59] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.514, eval_acc=0.932, eval_loss=31.9, eval_precision=0.452, eval_recall=0.596]\n",
      "Epoch: 7/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.649, train_acc=0.957, train_loss=19.8, train_precision=0.678, train_recall=0.631]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.584, eval_acc=0.947, eval_loss=27.3, eval_precision=0.54, eval_recall=0.635]\n",
      "Epoch: 8/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.648, train_acc=0.954, train_loss=20, train_precision=0.684, train_recall=0.632]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.64, eval_acc=0.954, eval_loss=22.6, eval_precision=0.61, eval_recall=0.673]  \n",
      "Epoch: 9/30 Train: 100%|██████████| 50/50 [00:42<00:00,  1.19it/s, F1=0.694, train_acc=0.965, train_loss=14.7, train_precision=0.702, train_recall=0.692]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.63, eval_acc=0.953, eval_loss=21.1, eval_precision=0.618, eval_recall=0.642] \n",
      "Epoch: 10/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.20it/s, F1=0.747, train_acc=0.97, train_loss=11.7, train_precision=0.757, train_recall=0.743] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.622, eval_acc=0.951, eval_loss=20.9, eval_precision=0.614, eval_recall=0.63] \n",
      "Epoch: 11/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.768, train_acc=0.975, train_loss=9.97, train_precision=0.773, train_recall=0.769]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.645, eval_acc=0.953, eval_loss=21, eval_precision=0.657, eval_recall=0.634]  \n",
      "Epoch: 12/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.20it/s, F1=0.775, train_acc=0.974, train_loss=10, train_precision=0.779, train_recall=0.777]  \n",
      "Eval Result: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it, F1=0.632, eval_acc=0.954, eval_loss=25.5, eval_precision=0.645, eval_recall=0.62] \n",
      "Epoch: 13/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.20it/s, F1=0.784, train_acc=0.976, train_loss=9.93, train_precision=0.791, train_recall=0.783]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.624, eval_acc=0.947, eval_loss=29.5, eval_precision=0.726, eval_recall=0.547]\n",
      "Epoch: 14/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.77, train_acc=0.973, train_loss=9.99, train_precision=0.779, train_recall=0.768] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.635, eval_acc=0.956, eval_loss=22.3, eval_precision=0.647, eval_recall=0.623]\n",
      "Epoch: 15/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.817, train_acc=0.984, train_loss=6.76, train_precision=0.821, train_recall=0.817]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.656, eval_acc=0.959, eval_loss=21.7, eval_precision=0.649, eval_recall=0.663]\n",
      "Epoch: 16/30 Train: 100%|██████████| 50/50 [00:42<00:00,  1.19it/s, F1=0.863, train_acc=0.989, train_loss=5.05, train_precision=0.861, train_recall=0.865]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.65, eval_acc=0.956, eval_loss=23.8, eval_precision=0.649, eval_recall=0.652]\n",
      "Epoch: 17/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.873, train_acc=0.99, train_loss=4.46, train_precision=0.871, train_recall=0.878] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.653, eval_acc=0.956, eval_loss=25.4, eval_precision=0.638, eval_recall=0.668]\n",
      "Epoch: 18/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.907, train_acc=0.993, train_loss=3.6, train_precision=0.903, train_recall=0.911] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.669, eval_acc=0.957, eval_loss=27.4, eval_precision=0.665, eval_recall=0.674]\n",
      "Epoch: 19/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.911, train_acc=0.993, train_loss=3.25, train_precision=0.911, train_recall=0.912]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.651, eval_acc=0.956, eval_loss=35.7, eval_precision=0.666, eval_recall=0.637]\n",
      "Epoch: 20/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.921, train_acc=0.994, train_loss=3.44, train_precision=0.919, train_recall=0.924]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.644, eval_acc=0.958, eval_loss=32.9, eval_precision=0.648, eval_recall=0.64] \n",
      "Epoch: 21/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.925, train_acc=0.995, train_loss=3.16, train_precision=0.926, train_recall=0.925]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.675, eval_acc=0.959, eval_loss=31.7, eval_precision=0.676, eval_recall=0.674]\n",
      "Epoch: 22/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.933, train_acc=0.994, train_loss=2.99, train_precision=0.932, train_recall=0.934]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.658, eval_acc=0.956, eval_loss=35, eval_precision=0.65, eval_recall=0.666]  \n",
      "Epoch: 23/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.924, train_acc=0.993, train_loss=3.25, train_precision=0.924, train_recall=0.926]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.64, eval_acc=0.952, eval_loss=28.6, eval_precision=0.611, eval_recall=0.671]\n",
      "Epoch: 24/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.20it/s, F1=0.915, train_acc=0.993, train_loss=3.38, train_precision=0.914, train_recall=0.918]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.663, eval_acc=0.949, eval_loss=28, eval_precision=0.677, eval_recall=0.649]  \n",
      "Epoch: 25/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.20it/s, F1=0.933, train_acc=0.995, train_loss=2.6, train_precision=0.931, train_recall=0.936] \n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.651, eval_acc=0.956, eval_loss=27.5, eval_precision=0.663, eval_recall=0.639]\n",
      "Epoch: 26/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.20it/s, F1=0.948, train_acc=0.996, train_loss=2.38, train_precision=0.949, train_recall=0.948]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, F1=0.65, eval_acc=0.955, eval_loss=24.8, eval_precision=0.674, eval_recall=0.629] \n",
      "Epoch: 27/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.20it/s, F1=0.949, train_acc=0.996, train_loss=2.25, train_precision=0.946, train_recall=0.953]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.632, eval_acc=0.952, eval_loss=27.2, eval_precision=0.613, eval_recall=0.653]\n",
      "Epoch: 28/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.20it/s, F1=0.954, train_acc=0.996, train_loss=2.34, train_precision=0.954, train_recall=0.955]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.65, eval_acc=0.953, eval_loss=26.8, eval_precision=0.646, eval_recall=0.654] \n",
      "Epoch: 29/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.19it/s, F1=0.928, train_acc=0.994, train_loss=3.11, train_precision=0.926, train_recall=0.931]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.647, eval_acc=0.956, eval_loss=27.6, eval_precision=0.645, eval_recall=0.649]\n",
      "Epoch: 30/30 Train: 100%|██████████| 50/50 [00:41<00:00,  1.20it/s, F1=0.941, train_acc=0.995, train_loss=2.61, train_precision=0.941, train_recall=0.942]\n",
      "Eval Result: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s, F1=0.676, eval_acc=0.957, eval_loss=27, eval_precision=0.679, eval_recall=0.674] \n"
     ]
    }
   ],
   "source": [
    "#四川\n",
    "# sc_400_baseline:0.6795、0.6949、0.6936 0.6868 0.6804\n",
    "# sc_ccks_dev_1 0.6831 0.6833 0.6804\n",
    "# sc_keyword_dev_1 预训练和文件的标签keyword 0.6868 0.6936 0.6788\n",
    "\n",
    "# sc_medicine_vocab_baseline 0.6802 0.6882 0.6824 0.7127 0.6702 0.6872\n",
    "# sc_medicine_vocab_pretrain_ccks 0.6647\n",
    "# sc_medicine_vocab_pretrain_keyword 0.6890\n",
    "\n",
    "# sc_tx_med_baseline 0.6820 0.6838 0.6917 0.6936 0.68\n",
    "\n",
    "# args[\"pretrained_file_name\"] = \"./model/chinese_wwm_ext/pytorch_model.bin\"\n",
    "args[\"pretrained_file_name\"] = \"./save_pretrained/sc_ccks_pretrain_3/Bert_13600/pytorch_model.bin\"\n",
    "# args[\"pretrained_file_name\"] = \"./save_pretrained/sc_keyword_pretrain_2/Bert_34000/pytorch_model.bin\"\n",
    "\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/tencent_vocab.txt\"\n",
    "args[\"word_vocab_file\"] =\"./data/FN/medicine_vocab.txt\"\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/tencent_medicine_vocab.txt\"\n",
    "\n",
    "args[\"task_name\"] = \"sc_medicine_pretrain_ccks_2\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 福建\n",
    "\n",
    "from CC.trainer import NERTrainer\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    # 'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    # 'pretrained_file_name': './save_pretrained/fj_ccks_pretrain_2/Bert_19040/pytorch_model.bin',\n",
    "    'pretrained_file_name': './save_pretrained/fj_keyword_pretrain_2/Bert_27200/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    # 'train_file': './data/FN/fj+sc/train(400).json',\n",
    "    'train_file': './data/FN/fj_json/train_400.json',\n",
    "    'eval_file': './data/FN/fj_json/dev.json',\n",
    "    'test_file': './data/FN/fj_json/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/FN/tags_list.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'output_eval':True,\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    # \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file\":\"./data/FN/medicine_vocab.txt\",\n",
    "    # \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    'model_name': 'LEBert',\n",
    "    # 'task_name': 'fj_medicine_vocab_keyword'\n",
    "}\n",
    "# trainer = NERTrainer(**args)\n",
    "\n",
    "# for i in trainer(lr2=1e-2):\n",
    "#     a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 福建\n",
    "# fj_baseline 0.9233\n",
    "# fj_keyword_dev_1 0.9039\n",
    "# fj_ccks_dev_1 0.9219\n",
    "\n",
    "# fj_medicine_vocab_baseline 0.9289\n",
    "# fj_medicine_vocab_ccks 0.9220\n",
    "# fj_medicine_vocab_keyword 0.9079\n",
    "\n",
    "args[\"pretrained_file_name\"] = \"./model/chinese_wwm_ext/pytorch_model.bin\"\n",
    "# args[\"pretrained_file_name\"] = \"./save_pretrained/fj_ccks_pretrain_2/Bert_19040/pytorch_model.binn\"\n",
    "# args[\"pretrained_file_name\"] = \"./save_pretrained/fj_keyword_pretrain_2/Bert_27200/pytorch_model.bin\"\n",
    "\n",
    "args[\"word_vocab_file\"] =\"./data/tencent/tencent_vocab.txt\"\n",
    "# args[\"word_vocab_file\"] =\"./data/FN/medicine_vocab.txt\"\n",
    "# args[\"word_vocab_file\"] =\"./data/tencent/tencent_medicine_vocab.txt\"\n",
    "\n",
    "args[\"task_name\"] = \"fj_baseline\"\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ccks\n",
    "# ccks_baseline 0.8258 0.8197 \n",
    "# ccks_medicine_vocab_baseline 0.8149 0.8120\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/ccks/train.json',\n",
    "    'eval_file': './data/ccks/dev.json',\n",
    "    'test_file': './data/ccks/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/ccks/ccks_tags_list.txt',\n",
    "    'output_eval': True,\n",
    "    'loader_name': 'le_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    # \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file\":\"./data/FN/medicine_vocab.txt0\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'ccks_medicine_vocab_baseline_2'\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9392d1f0914889243d058bb73f0d89e61311fd6d751bbc8fa50e38d7d4ff811"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('NER': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
