{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小样本实验\n",
    "数据规模：0.25k 0.5k 1k 1.35k  \n",
    "抽样方式：随机抽样，设置统一的样本种子   \n",
    "结果：每个模型训练3次，取最好的一次作为结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据样本\n",
    "import os\n",
    "\n",
    "path = \"data/few_shot\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 微博数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制微博数据\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "data = [\"train.json\",\"dev.json\",\"test.json\",\"labels.txt\"]\n",
    "\n",
    "if not os.path.exists(\"data/few_shot/weibo\"):\n",
    "    os.makedirs(\"data/few_shot/weibo\")\n",
    "for f in data:\n",
    "    copyfile(f\"data/weibo/{f}\",f\"data/few_shot/weibo/{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽样数据\n",
    "postfix = [250,500,1000,1350]\n",
    "\n",
    "# 设置统一的随机种子\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "# 读取原始训练集\n",
    "train_file = \"data/few_shot/weibo/train.json\"\n",
    "train_data = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "with open(train_file,\"r\",encoding=\"utf-8\") as reader:\n",
    "    for line in tqdm(reader,desc=f\"load {train_file}\"):\n",
    "        data = json.loads(line.strip())\n",
    "        text,label = data[\"text\"],data[\"label\"]\n",
    "        assert len(text)==len(label)\n",
    "        if len(text)>0:\n",
    "            train_data.append(data)\n",
    "        else:\n",
    "            print(text,label)\n",
    "\n",
    "# 重置随机，确保结果可复现\n",
    "random.seed(2021)\n",
    "\n",
    "# 打乱训练集\n",
    "shuffle(train_data)\n",
    "\n",
    "# 生成数据\n",
    "for post in postfix:\n",
    "    data = train_data[:post]\n",
    "    assert len(data)==post\n",
    "    with open(f\"data/few_shot/weibo/train_{post}.json\",\"w\",encoding=\"utf-8\") as f: \n",
    "        for line in tqdm(data,desc=f\"{post}\"):\n",
    "            f.write(f\"{json.dumps(line,ensure_ascii=False)}\\n\")\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notonotes4 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制Ontonotes4数据\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "data = [\"train.json\",\"dev.json\",\"test.json\",\"labels.txt\"]\n",
    "\n",
    "if not os.path.exists(\"data/few_shot/note4\"):\n",
    "    os.makedirs(\"data/few_shot/note4\")\n",
    "for f in data:\n",
    "    copyfile(f\"data/lebert/dataset/NER/note4/{f}\",f\"data/few_shot/note4/{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽样数据\n",
    "postfix = [250,500,1000,1350]\n",
    "\n",
    "# 设置统一的随机种子\n",
    "import random\n",
    "\n",
    "# 读取原始训练集\n",
    "train_file = \"data/few_shot/note4/train.json\"\n",
    "train_data = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "with open(train_file,\"r\",encoding=\"utf-8\") as reader:\n",
    "    for line in tqdm(reader,desc=f\"load {train_file}\"):\n",
    "        data = json.loads(line.strip())\n",
    "        text,label = data[\"text\"],data[\"label\"]\n",
    "        assert len(text)==len(label)\n",
    "        if len(text)>0:\n",
    "            train_data.append(data)\n",
    "        else:\n",
    "            print(text,label)\n",
    "\n",
    "# 重置随机，确保结果可复现\n",
    "random.seed(2021)\n",
    "\n",
    "# 打乱训练集\n",
    "shuffle(train_data)\n",
    "\n",
    "# 生成数据\n",
    "for post in postfix:\n",
    "    data = train_data[:post]\n",
    "    assert len(data)==post\n",
    "    with open(f\"data/few_shot/note4/train_{post}.json\",\"w\",encoding=\"utf-8\") as f: \n",
    "        for line in tqdm(data,desc=f\"{post}\"):\n",
    "            f.write(f\"{json.dumps(line,ensure_ascii=False)}\\n\")\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MSRA 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制MSRA数据\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "data = [\"train.json\",\"dev.json\",\"test.json\",\"labels.txt\"]\n",
    "\n",
    "if not os.path.exists(\"data/few_shot/msra\"):\n",
    "    os.makedirs(\"data/few_shot/msra\")\n",
    "for f in data:\n",
    "    copyfile(f\"data/lebert/dataset/NER/msra/{f}\",f\"data/few_shot/msra/{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽样数据\n",
    "postfix = [250,500,1000,1350]\n",
    "\n",
    "# 设置统一的随机种子\n",
    "import random\n",
    "\n",
    "# 读取原始训练集\n",
    "train_file = \"data/few_shot/msra/train.json\"\n",
    "train_data = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "with open(train_file,\"r\",encoding=\"utf-8\") as reader:\n",
    "    for line in tqdm(reader,desc=f\"load {train_file}\"):\n",
    "        data = json.loads(line.strip())\n",
    "        text,label = data[\"text\"],data[\"label\"]\n",
    "        assert len(text)==len(label)\n",
    "        if len(text)>0:\n",
    "            train_data.append(data)\n",
    "        else:\n",
    "            print(text,label)\n",
    "\n",
    "# 重置随机，确保结果可复现\n",
    "random.seed(2021)\n",
    "\n",
    "# 打乱训练集\n",
    "shuffle(train_data)\n",
    "\n",
    "# 生成数据\n",
    "for post in postfix:\n",
    "    data = train_data[:post]\n",
    "    assert len(data)==post\n",
    "    with open(f\"data/few_shot/msra/train_{post}.json\",\"w\",encoding=\"utf-8\") as f: \n",
    "        for line in tqdm(data,desc=f\"{post}\"):\n",
    "            f.write(f\"{json.dumps(line,ensure_ascii=False)}\\n\")\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resume数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制Resume数据\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "data = [\"train.json\",\"dev.json\",\"test.json\",\"labels.txt\"]\n",
    "\n",
    "if not os.path.exists(\"data/few_shot/resume\"):\n",
    "    os.makedirs(\"data/few_shot/resume\")\n",
    "for f in data:\n",
    "    copyfile(f\"data/lebert/dataset/NER/resume/{f}\",f\"data/few_shot/resume/{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽样数据\n",
    "postfix = [250,500,1000,1350]\n",
    "\n",
    "# 设置统一的随机种子\n",
    "import random\n",
    "\n",
    "# 读取原始训练集\n",
    "train_file = \"data/few_shot/resume/train.json\"\n",
    "train_data = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "with open(train_file,\"r\",encoding=\"utf-8\") as reader:\n",
    "    for line in tqdm(reader,desc=f\"load {train_file}\"):\n",
    "        data = json.loads(line.strip())\n",
    "        text,label = data[\"text\"],data[\"label\"]\n",
    "        assert len(text)==len(label)\n",
    "        if len(text)>0:\n",
    "            train_data.append(data)\n",
    "        else:\n",
    "            print(text,label)\n",
    "\n",
    "# 重置随机，确保结果可复现\n",
    "random.seed(2021)\n",
    "\n",
    "# 打乱训练集\n",
    "shuffle(train_data)\n",
    "\n",
    "# 生成数据\n",
    "for post in postfix:\n",
    "    data = train_data[:post]\n",
    "    assert len(data)==post\n",
    "    with open(f\"data/few_shot/resume/train_{post}.json\",\"w\",encoding=\"utf-8\") as f: \n",
    "        for line in tqdm(data,desc=f\"{post}\"):\n",
    "            f.write(f\"{json.dumps(line,ensure_ascii=False)}\\n\")\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert+LSTM+CRF(Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    \"model_name\":\"Bert\",\n",
    "    \"loader\":\"cn_loader\",\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'hidden_dim': 512,\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 256,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/few_shot/weibo/train_250.json',\n",
    "    'eval_file': 'data/few_shot/weibo/dev.json',\n",
    "    'test_file': 'data/few_shot/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/few_shot/weibo/labels.txt',\n",
    "    'loader_name': 'cn_loader',\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'use_json': True,\n",
    "    'model_name': 'Bert',\n",
    "    'output_eval': True,\n",
    "    'task_name': 'weibo_bert_crf_250_x3',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "per_task_count = 1\n",
    "\n",
    "datasets = [\"weibo\",\"note4\",\"msra\",\"resume\"]\n",
    "\n",
    "task_trainset = [250,500,1000,1350]\n",
    "\n",
    "tasks_args = {\n",
    "        \"train_file\":\"data/few_shot/{}/train_{}.json\",\n",
    "        \"eval_file\":\"data/few_shot/{}/dev.json\",\n",
    "        \"test_file\":\"data/few_shot/{}/test.json\",\n",
    "        'tag_file':\"data/few_shot/{}/labels.txt\",\n",
    "        \"task_name\":\"{}_bert_crf_{}_{}\",\n",
    "}\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "for name in datasets:\n",
    "    for trainset in task_trainset:\n",
    "        for i in range(per_task_count):\n",
    "            for key,value in tasks_args.items():\n",
    "                model_args[key] = value.format(name,trainset,f\"x{i+1}\")\n",
    "            set_seed(2021)\n",
    "            trainer = NERTrainer(**model_args)\n",
    "\n",
    "            for _ in trainer.train():\n",
    "                pass\n",
    "\n",
    "            torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEBert+LSTM+CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    \"loader\":\"le_loader\",\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'hidden_dim': 512,\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 256,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/few_shot/weibo/train_250.json',\n",
    "    'eval_file': 'data/few_shot/weibo/dev.json',\n",
    "    'test_file': 'data/few_shot/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'word_embedding_file': 'data/tencent/word_embedding.txt',\n",
    "    'tag_file': 'data/few_shot/weibo/labels.txt',\n",
    "    'word_vocab_file':'data/tencent/tencent_vocab.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'use_json': True,\n",
    "    'model_name': 'LEBert',\n",
    "    'output_eval': True,\n",
    "    'task_name': 'weibo_lebert_crf_250_x3',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "per_task_count = 1\n",
    "\n",
    "datasets = [\"weibo\",\"note4\",\"msra\",\"resume\"]\n",
    "\n",
    "task_trainset = [250,500,1000,1350]\n",
    "\n",
    "tasks_args = {\n",
    "        \"train_file\":\"data/few_shot/{}/train_{}.json\",\n",
    "        \"eval_file\":\"data/few_shot/{}/dev.json\",\n",
    "        \"test_file\":\"data/few_shot/{}/test.json\",\n",
    "        'tag_file':\"data/few_shot/{}/labels.txt\",\n",
    "        \"task_name\":\"{}_lebert_crf_{}_{}\"\n",
    "}\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "for name in datasets:\n",
    "    for trainset in task_trainset:\n",
    "        for i in range(per_task_count):\n",
    "            for key,value in tasks_args.items():\n",
    "                model_args[key] = value.format(name,trainset,f\"x{i+1}\")\n",
    "            set_seed(2021)\n",
    "            trainer = NERTrainer(**model_args)\n",
    "\n",
    "            for _ in trainer.train():\n",
    "                pass\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompts PreTrain+LeBert FineTune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weibo\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "pretrain_model_args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/few_shot/weibo/train_250.json',\n",
    "    'eval_file': 'data/few_shot/weibo/dev.json',\n",
    "    'test_file': 'data/few_shot/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/few_shot/weibo/labels.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'weibo_pretrain_lebert_250_pretraind_task',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\":\"非实体\",\n",
    "        \"PER.NOM\": \"指代人名\",\n",
    "        \"LOC.NAM\": \"地名\",\n",
    "        \"PER.NAM\": \"人名\",\n",
    "        \"GPE.NAM\": \"政体\",\n",
    "        \"ORG.NAM\": \"机构\",\n",
    "        \"ORG.NOM\": \"指代机构\",\n",
    "        \"LOC.NOM\": \"指代地名\",\n",
    "        \"GPE.NOM\": \"指代政体\",\n",
    "        \"ORG\": \"组织\",\n",
    "        \"LOC\": \"地点\",\n",
    "        \"PER\": \"人\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"测量单位\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "trainsets = [250,500,1000,1350]\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "for train in trainsets:\n",
    "    pretrain_model_args['train_file'] = f'data/few_shot/weibo/train_{train}.json'\n",
    "    pretrain_model_args['task_name'] = f'weibo_pretrain_lebert_{train}_pretraind_task'\n",
    "    set_seed(2021)\n",
    "    pre_trainer = NERPreTrainer(**pretrain_model_args)\n",
    "\n",
    "    for i in pre_trainer():\n",
    "        a = i\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note4\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "import torch\n",
    "\n",
    "pretrain_model_args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/few_shot/note4/train_250.json',\n",
    "    'eval_file': 'data/few_shot/note4/dev.json',\n",
    "    'test_file': 'data/few_shot/note4/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/few_shot/note4/labels.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'note4_pretrain_lebert_250_pretraind_task',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\":\"非实体\",\n",
    "        \"ORG\": \"组织\",\n",
    "        \"LOC\": \"地点\",\n",
    "        \"PER\": \"人\",\n",
    "        \"GPE\": \"政治\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"测量单位\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "trainsets = [250,500,1000,1350]\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "for train in trainsets:\n",
    "    pretrain_model_args['train_file'] = f'data/few_shot/note4/train_{train}.json'\n",
    "    pretrain_model_args['task_name'] = f'note4_pretrain_lebert_{train}_pretraind_task'\n",
    "    set_seed(2021)\n",
    "    pre_trainer = NERPreTrainer(**pretrain_model_args)\n",
    "\n",
    "    for i in pre_trainer():\n",
    "        a = i\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msra\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "import torch\n",
    "\n",
    "pretrain_model_args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/few_shot/msra/train_250.json',\n",
    "    'eval_file': 'data/few_shot/msra/dev.json',\n",
    "    'test_file': 'data/few_shot/msra/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/few_shot/msra/labels.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'msra_pretrain_lebert_250_pretraind_task',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\":\"非实体\",\n",
    "        \"NS\":\"地名\",\n",
    "        \"NR\":\"人名\",\n",
    "        \"NT\":\"机构团体\",\n",
    "        \"ORG\": \"组织\",\n",
    "        \"LOC\": \"地点\",\n",
    "        \"PER\": \"人\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"测量单位\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "trainsets = [250,500,1000,1350]\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "for train in trainsets:\n",
    "    pretrain_model_args['train_file'] = f'data/few_shot/msra/train_{train}.json'\n",
    "    pretrain_model_args['task_name'] = f'msra_pretrain_lebert_{train}_pretraind_task'\n",
    "    set_seed(2021)\n",
    "    pre_trainer = NERPreTrainer(**pretrain_model_args)\n",
    "\n",
    "    for i in pre_trainer():\n",
    "        a = i\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "import torch\n",
    "\n",
    "pretrain_model_args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/few_shot/resume/train_250.json',\n",
    "    'eval_file': 'data/few_shot/resume/dev.json',\n",
    "    'test_file': 'data/few_shot/resume/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/few_shot/resume/labels.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'resume_pretrain_lebert_250_pretraind_task',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\":\"非实体\",\n",
    "        \"NAME\":\"名字\",\n",
    "        \"CONT\":\"国家\",\n",
    "        \"RACE\":\"种族背景\",\n",
    "        \"TITLE\":\"职位\",\n",
    "        \"EDU\":\"教育机构\",\n",
    "        \"PRO\":\"专业\",\n",
    "        \"ORG\": \"组织\",\n",
    "        \"LOC\": \"地点\",\n",
    "        \"PER\": \"人\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"测量单位\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "trainsets = [250,500,1000,1350]\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "for train in trainsets:\n",
    "    pretrain_model_args['train_file'] = f'data/few_shot/resume/train_{train}.json'\n",
    "    pretrain_model_args['task_name'] = f'resume_pretrain_lebert_{train}_pretraind_task'\n",
    "    set_seed(2021)\n",
    "    pre_trainer = NERPreTrainer(**pretrain_model_args)\n",
    "\n",
    "    for i in pre_trainer():\n",
    "        a = i\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    \"loader\":\"le_loader\",\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'hidden_dim': 512,\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 256,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/few_shot/weibo/train_250.json',\n",
    "    'eval_file': 'data/few_shot/weibo/dev.json',\n",
    "    'test_file': 'data/few_shot/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'word_embedding_file': 'data/tencent/word_embedding.txt',\n",
    "    'tag_file': 'data/few_shot/weibo/labels.txt',\n",
    "    'word_vocab_file':'data/tencent/tencent_vocab.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'use_json': True,\n",
    "    'model_name': 'LEBert',\n",
    "    'output_eval': True,\n",
    "    'task_name': 'weibo_lebert_crf_250_x3',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "import torch\n",
    "\n",
    "per_task_count = 1\n",
    "\n",
    "datasets = [\"weibo\",\"note4\",\"msra\",\"resume\"]\n",
    "\n",
    "task_trainset = [250,500,1000,1350]\n",
    "\n",
    "pretrain = [240,480,960,1290]\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# dataset,counts,time,others\n",
    "tasks_args = {\n",
    "        'pretrained_file_name': 'save_pretrained/{0}_pretrain_lebert_{1}_pretraind_task/Bert_{3}/pytorch_model.bin',\n",
    "        \"train_file\":\"data/few_shot/{0}/train_{1}.json\",\n",
    "        \"eval_file\":\"data/few_shot/{0}/dev.json\",\n",
    "        \"test_file\":\"data/few_shot/{0}/test.json\",\n",
    "        'tag_file':\"data/few_shot/{0}/labels.txt\",\n",
    "        \"task_name\":\"{0}_pretrain_lebert_crf_{1}_{2}\"\n",
    "        \n",
    "}\n",
    "\n",
    "for name in datasets:\n",
    "    for trainset,pre in zip(task_trainset,pretrain): \n",
    "        for i in range(per_task_count):\n",
    "            for key,value in tasks_args.items():\n",
    "                model_args[key] = value.format(name,trainset,f\"x{i+1}\",pre)\n",
    "            set_seed(2021)\n",
    "            trainer = NERTrainer(**model_args)\n",
    "\n",
    "            for _ in trainer.train():\n",
    "                pass\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position Prompts(P-Bert) + Position Prompts with C-Bert(PC-BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造预测训练集\n",
    "# 使用以SuperNER的LEBert作为预测模型\n",
    "\n",
    "# 预测Train.json\n",
    "# weibo train.json\n",
    "from CC.predicter import NERPredict\n",
    "import json\n",
    "\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/SuperNER_note4/train.json',\n",
    "    'eval_file': 'data/weibo/dev.json',\n",
    "    'test_file': 'data/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/SuperNER_note4/labels.txt',\n",
    "    'output_eval': True,\n",
    "    'loader_name': 'le_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 64,\n",
    "    'eval_batch_size': 512,\n",
    "    'do_shuffle': True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'origin_super_note4_predict_model'\n",
    "}\n",
    "\n",
    "# 使用了预训练模型\n",
    "args[\"lstm_crf_model_file\"] = \"save_model/super_predict_model/lstm_crf/lstm_crf_66930.pth\"\n",
    "args[\"bert_model_file\"] = \"save_model/super_predict_model/LEBert/LEBert_66930.pth\"\n",
    "predict = NERPredict(**args)\n",
    "\n",
    "\n",
    "def predict_train(train_file, saved_file):\n",
    "    batch_size = 64\n",
    "    index = 0\n",
    "    sentences = []\n",
    "\n",
    "    with open(saved_file, \"w\", encoding=\"utf-8\") as out:\n",
    "        with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                text = data[\"text\"]\n",
    "\n",
    "                sentences.append(text)\n",
    "                index += 1\n",
    "                if index % batch_size == batch_size-1:\n",
    "                    for s, label in predict(sentences):\n",
    "                        assert len(s[:args[\"max_seq_length\"]-2]) == len(label),f\"{s} {label} {len(s)} {len(label)}\"\n",
    "                        out.write(\n",
    "                            f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")\n",
    "                    sentences = []\n",
    "                    out.flush()\n",
    "            if len(sentences) > 0:\n",
    "                for s, label in predict(sentences):\n",
    "                    assert len(s[:args[\"max_seq_length\"]]) == len(label),f\"{s} {label} {len(s)} {len(label)}\"\n",
    "                    out.write(\n",
    "                        f\"\"\"{json.dumps({\"text\":s[:args[\"max_seq_length\"]-2],\"label\":label},ensure_ascii=False)}\\n\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsets = [\"weibo\",\"note4\",\"resume\",\"msra\"]\n",
    "scales = [250,500,1000,1350]\n",
    "path = f\"data/few_shot\"\n",
    "\n",
    "for dataname in trainsets:\n",
    "    for scale in scales:\n",
    "        train_file = f\"{path}/{dataname}/train_{scale}.json\"\n",
    "        saved_file = f\"{path}/{dataname}/train_{scale}_super_note4_pred.json\"\n",
    "        predict_train(train_file,saved_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造超集标签\n",
    "\n",
    "# 合并标签\n",
    "from tools.merge_json import merge_labels\n",
    "\n",
    "labels_origin = [\n",
    "    \"data/few_shot/weibo/labels.txt\",\n",
    "    \"data/few_shot/note4/labels.txt\",\n",
    "    \"data/few_shot/resume/labels.txt\",\n",
    "    \"data/few_shot/msra/labels.txt\"\n",
    "]\n",
    "\n",
    "labels = [\"data/SuperNER/tags_list.txt\",\"data/lebert/dataset/NER/note4/labels.txt\"]\n",
    "\n",
    "for origin in labels_origin:\n",
    "    merge_labels([origin]+labels,origin.replace(\"labels.txt\",\"super_labels.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LTS branch\n",
    "pretrain_model_args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/few_shot/weibo/train_250_super_note4_pred.json',\n",
    "    'eval_file': './data/few_shot/weibo/dev.json',\n",
    "    'test_file': './data/few_shot/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/few_shot/weibo/super_labels.txt',\n",
    "    'loader_name': 'ptloader_v2',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'weibo_pbert_pretrain_250',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"PER.NOM\": \"指代人名\",\n",
    "        \"LOC.NAM\": \"地名\",\n",
    "        \"PER.NAM\": \"人名\",\n",
    "        \"GPE.NAM\": \"政体\",\n",
    "        \"ORG.NAM\": \"机构\",\n",
    "        \"ORG.NOM\": \"指代机构\",\n",
    "        \"LOC.NOM\": \"指代地名\",\n",
    "        \"GPE.NOM\": \"指代政体\",\n",
    "        \"NR\": \"人名\",\n",
    "        \"NS\": \"地名\",\n",
    "        \"NT\": \"组织机构\",\n",
    "        \"CONT\": \"国家\",\n",
    "        \"PRO\": \"职位\",\n",
    "        \"RACE\": \"种族\",\n",
    "        \"TITLE\": \"工作名称\",\n",
    "        \"EDU\": \"教育经历\",\n",
    "        \"NAME\": \"名字\",\n",
    "        \"ORG\": \"机构\",\n",
    "        \"LOC\": \"地名\",\n",
    "        \"PER\": \"人名\",\n",
    "        \"GPE\": \"政治实体\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"度量\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\",\n",
    "        \"NORP\": \"政体民族\",\n",
    "        \"PERSON\": \"人名\",\n",
    "    }\n",
    "}\n",
    "\n",
    "import torch\n",
    "from CC.pre_trained import NERPreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"weibo\",\"note4\",\"resume\",\"msra\"]\n",
    "\n",
    "scales = [250,500,1000,1350]\n",
    "\n",
    "tasks_args = {\n",
    "        \"train_file\":\"./data/few_shot/{0}/train_{1}_super_note4_pred.json\",\n",
    "        \"eval_file\":'./data/few_shot/{0}/dev.json',\n",
    "        \"test_file\":'./data/few_shot/{0}/test.json',\n",
    "        \"tag_file\":\"data/few_shot/{0}/super_labels.txt\",\n",
    "        'task_name': '{0}_pbert_pretrain_{1}'\n",
    "    }\n",
    "\n",
    "\n",
    "for name in datasets:\n",
    "    for scale in scales:\n",
    "        for k,v in tasks_args.items():\n",
    "            pretrain_model_args[k] = v.format(name,scale)\n",
    "        pre_trainer = NERPreTrainer(**pretrain_model_args)\n",
    "\n",
    "        for i in pre_trainer():\n",
    "            a = i\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2,3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': '.save_pretrained/weibo_multiple_pretrained/Bert_5915/pytorch_model.bin',\n",
    "    'prompt_pretrained_file_name': 'save_pretrained/weibo_multiple_pretrained/Bert_5915/pytorch_model.bin',\n",
    "    'prompt_config_file_name': 'save_pretrained/weibo_multiple_pretrained/Bert_5915/config.json',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/weibo/train.json',\n",
    "    'eval_file': './data/weibo/dev.json',\n",
    "    'test_file': './data/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/weibo/labels.txt',\n",
    "    'output_eval': True,\n",
    "    'loader_name': 'ft_loader_v4',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 32,\n",
    "    'do_shuffle': True,\n",
    "    'model_name': 'LEBert',\n",
    "    \"tag_rules\": {\n",
    "        \"O\": \"非实体\",\n",
    "        \"PER.NOM\": \"指代人名\",\n",
    "        \"LOC.NAM\": \"地名\",\n",
    "        \"PER.NAM\": \"人名\",\n",
    "        \"GPE.NAM\": \"政体\",\n",
    "        \"ORG.NAM\": \"机构\",\n",
    "        \"ORG.NOM\": \"指代机构\",\n",
    "        \"LOC.NOM\": \"指代地名\",\n",
    "        \"GPE.NOM\": \"指代政体\",\n",
    "        \"NR\": \"人名\",\n",
    "        \"NS\": \"地名\",\n",
    "        \"NT\": \"组织机构\",\n",
    "        \"CONT\": \"国家\",\n",
    "        \"PRO\": \"职位\",\n",
    "        \"RACE\": \"种族\",\n",
    "        \"TITLE\": \"工作名称\",\n",
    "        \"EDU\": \"教育经历\",\n",
    "        \"NAME\": \"名字\",\n",
    "        \"ORG\": \"机构\",\n",
    "        \"LOC\": \"地名\",\n",
    "        \"PER\": \"人名\",\n",
    "        \"GPE\": \"政治实体\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"度量\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\",\n",
    "        \"NORP\": \"政体民族\",\n",
    "        \"PERSON\": \"人名\",\n",
    "    },\n",
    "    'task_name': 'weibo_tag_multiple_3'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"note4\"]\n",
    "\n",
    "scales = [250,500,1000,1350]\n",
    "\n",
    "steps = [1120,2205,4375,5915]\n",
    "\n",
    "per_task_count = 2\n",
    "\n",
    "tasks_args = {\n",
    "    \"task_name\": \"{0}_pc_bert_crf_{1}_{2}\",\n",
    "    'prompt_config_file_name':\"save_pretrained/{0}_pbert_pretrain_{1}/Bert_{3}/config.json\",\n",
    "    \"pretrained_file_name\":\"save_pretrained/{0}_pbert_pretrain_{1}/Bert_{3}/pytorch_model.bin\",\n",
    "    'prompt_pretrained_file_name':\"save_pretrained/{0}_pbert_pretrain_{1}/Bert_{3}/pytorch_model.bin\",\n",
    "    'train_file': './data/few_shot/{0}/train_{1}.json',\n",
    "    'eval_file': './data/few_shot/{0}/dev.json',\n",
    "    'test_file': './data/few_shot/{0}/test.json',\n",
    "    'tag_file':\"./data/few_shot/{0}/labels.txt\"\n",
    "}\n",
    "from CC.enhanced_trainer import EnhancedNERTrainer\n",
    "import torch\n",
    "\n",
    "for dataset in datasets:\n",
    "    for scale,step in zip(scales,steps):\n",
    "        for i in range(per_task_count):\n",
    "            for k,v in tasks_args.items():\n",
    "                model_args[k]=v.format(dataset,scale,f\"x{i}\",step)\n",
    "            trainer = EnhancedNERTrainer(**model_args)\n",
    "\n",
    "            for _ in trainer(lr2=1e-2):\n",
    "                pass\n",
    "\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9627571863819468058fb8a0d45e3ad069ccb5b5ca291368ca8fe24c04521c7e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ccner': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
