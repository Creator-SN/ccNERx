{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from CC.predicter import NERPredict\n",
    "from CC.trainer import NERTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune 参数设置\n",
    "\n",
    "适用于**le_loader**下的fine-tune任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/weibo/train.json',\n",
    "    'eval_file': './data/weibo/dev.json',\n",
    "    'test_file': './data/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/weibo/labels.txt',\n",
    "    'output_eval': True,\n",
    "    'loader_name': 'le_loader',\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 64,\n",
    "    'eval_batch_size': 512,\n",
    "    'do_shuffle': True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'weibo'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数微调\n",
    "\n",
    "调整`Pretrained`和`task name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['pretrained_file_name'] = './save_pretrained/Weibo_Super1x_Pretrained/Bert_2550/pytorch_model.bin'\n",
    "args['train_file'] = './data/ontonotes5_s/train_02_1.json'\n",
    "args['eval_file'] = './data/ontonotes5_s/dev.json'\n",
    "args['test_file'] = './data/ontonotes5_s/test.json'\n",
    "args['tag_file'] = './data/ontonotes5_s/labels.txt'\n",
    "args['batch_size'] = 32\n",
    "args['task_name'] = 'ontonotes5_s_02_p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLEBert Fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune 参数设置\n",
    "\n",
    "适用于**le_loader**下的fine-tune任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from CC.predicter import NERPredict\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "# %%\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/ontonotes5/train.json',\n",
    "    'eval_file': './data/ontonotes5/dev.json',\n",
    "    'test_file': './data/ontonotes5/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/ontonotes5/ontonotes5_labels.txt',\n",
    "    'output_eval': True,\n",
    "    'loader_name': 'm_labelle_loader',\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"tag_embedding_file\":\"./data/tencent/label_embedding.txt\",\n",
    "    \"external_entities_file\": \"./data/X/data.json\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 64,\n",
    "    'eval_batch_size': 512,\n",
    "    'do_shuffle': True,\n",
    "    'model_name': 'PLEBert',\n",
    "    'task_name': 'weibo'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数微调\n",
    "\n",
    "调整`Pretrained`和`task name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['pretrained_file_name'] = './model/chinese_wwm_ext/pytorch_model.bin'\n",
    "args['train_file'] = './data/ontonotes5_s/train_02_1.json'\n",
    "args['eval_file'] = './data/ontonotes5_s/dev.json'\n",
    "args['test_file'] = './data/ontonotes5_s/test.json'\n",
    "args['tag_file'] = './data/ontonotes5_s/labels.txt'\n",
    "args['batch_size'] = 32\n",
    "args['task_name'] = 'ontonotes5_s_02_pc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "calculate ./data/ontonotes5_s/train_02_1.json etag: 100%|██████████| 1.24M/1.24M [00:00<00:00, 254MB/s]\n",
      "calculate ./data/ontonotes5_s/dev.json etag: 100%|██████████| 2.53M/2.53M [00:00<00:00, 307MB/s]\n",
      "calculate ./data/ontonotes5_s/test.json etag: 100%|██████████| 2.51M/2.51M [00:00<00:00, 352MB/s]\n",
      "calculate ./data/ontonotes5_s/labels.txt etag: 100%|██████████| 110/110 [00:00<00:00, 154kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 32,\n",
      "    \"eval_batch_size\": 512,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"tag_embedding_file\": \"./data/tencent/bert_label_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"external_entities_file\": \"./data/X/data.json\",\n",
      "    \"train_file\": \"./data/ontonotes5_s/train_02_1.json\",\n",
      "    \"eval_file\": \"./data/ontonotes5_s/dev.json\",\n",
      "    \"test_file\": \"./data/ontonotes5_s/test.json\",\n",
      "    \"tag_file\": \"./data/ontonotes5_s/labels.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 150,\n",
      "    \"max_word_num\": 5,\n",
      "    \"max_label_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"ontonotes5_s_02_pc\",\n",
      "    \"ignore_rules\": null\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/e8f00bac3c46f8256b55702fef095739_78e6ea77f57ba7f0bd789b863d786fd2_ce9c592468b96a018ce16213d4861974_a83264c5149f26d36ffa7fa77eac7624/1000000/lexicon_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "count line size ./data/ontonotes5_s/labels.txt: 18L [00:00, 146882.24L/s]\n",
      "build line mapper: 18L [00:00, 187804.66L/s]\n",
      "load vocab from files: 100%|██████████| 18/18 [00:00<00:00, 6072.83it/s]\n",
      "load vocab from list: 100%|██████████| 17/17 [00:00<00:00, 193233.52it/s]\n",
      "count line size ./data/tencent/bert_label_embedding.txt: 39L [00:00, 44852.72L/s]\n",
      "build line mapper: 39L [00:00, 20460.02L/s]\n",
      "load vocab from files: 100%|██████████| 38/38 [00:00<00:00, 4644.04it/s]\n",
      "load vocab from list: 100%|██████████| 37/37 [00:00<00:00, 318010.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached ./temp/e8f00bac3c46f8256b55702fef095739_78e6ea77f57ba7f0bd789b863d786fd2_ce9c592468b96a018ce16213d4861974_a83264c5149f26d36ffa7fa77eac7624/1000000/matched_words\n",
      "load cached ./temp/e8f00bac3c46f8256b55702fef095739_78e6ea77f57ba7f0bd789b863d786fd2_ce9c592468b96a018ce16213d4861974_a83264c5149f26d36ffa7fa77eac7624/1000000/word_vocab\n",
      "load cached ./temp/e8f00bac3c46f8256b55702fef095739_78e6ea77f57ba7f0bd789b863d786fd2_ce9c592468b96a018ce16213d4861974_a83264c5149f26d36ffa7fa77eac7624/1000000/vocab_embedding\n",
      "load cached ./temp/e8f00bac3c46f8256b55702fef095739_78e6ea77f57ba7f0bd789b863d786fd2_ce9c592468b96a018ce16213d4861974_a83264c5149f26d36ffa7fa77eac7624/1000000/tag_vocab_embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/lpc/miniconda3/envs/pcpower/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1642: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "load dataset from ./data/ontonotes5_s/train_02_1.json: 100%|██████████| 3131/3131 [00:02<00:00, 1305.57it/s]\n",
      "load dataset from ./data/ontonotes5_s/dev.json: 100%|██████████| 4301/4301 [00:04<00:00, 925.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing PLEBertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing PLEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PLEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of PLEBertModel were not initialized from the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin and are newly initialized: ['bert.encoder.layer.0.label_label_weight.weight', 'bert.encoder.layer.0.word_label_transform.bias', 'bert.embeddings.position_ids', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.label_score_weight.bias', 'label_embeddings.weight', 'bert.encoder.layer.0.attn_Label_W', 'bert.encoder.layer.0.word_transform.bias', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_label_transform.weight', 'bert.encoder.layer.0.label_score_weight.weight', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.label_label_weight.bias', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.word_transform.weight', 'word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train:  15%|█▌        | 15/98 [00:18<00:54,  1.51it/s, F1=0, train_acc=0.378, train_loss=88, train_precision=0, train_recall=0]/home/lpc/miniconda3/envs/pcpower/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch: 1/30 Train: 100%|██████████| 98/98 [01:11<00:00,  1.37it/s, F1=0.0691, train_acc=0.844, train_loss=24.2, train_precision=0.0592, train_recall=0.0847]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.09s/it, F1=0.408, eval_acc=0.944, eval_loss=9.15, eval_precision=0.348, eval_recall=0.494]\n",
      "Epoch: 2/30 Train: 100%|██████████| 98/98 [01:03<00:00,  1.55it/s, F1=0.556, train_acc=0.967, train_loss=3.58, train_precision=0.5, train_recall=0.635]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.08s/it, F1=0.675, eval_acc=0.961, eval_loss=5.61, eval_precision=0.608, eval_recall=0.765]\n",
      "Epoch: 3/30 Train: 100%|██████████| 98/98 [01:06<00:00,  1.48it/s, F1=0.735, train_acc=0.98, train_loss=1.98, train_precision=0.687, train_recall=0.795]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.09s/it, F1=0.677, eval_acc=0.96, eval_loss=5.71, eval_precision=0.674, eval_recall=0.684]\n",
      "Epoch: 4/30 Train: 100%|██████████| 98/98 [01:03<00:00,  1.54it/s, F1=0.846, train_acc=0.987, train_loss=1.25, train_precision=0.817, train_recall=0.881]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.09s/it, F1=0.751, eval_acc=0.967, eval_loss=5.06, eval_precision=0.735, eval_recall=0.773]\n",
      "Epoch: 5/30 Train: 100%|██████████| 98/98 [01:01<00:00,  1.58it/s, F1=0.888, train_acc=0.992, train_loss=0.819, train_precision=0.867, train_recall=0.913]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.09s/it, F1=0.755, eval_acc=0.966, eval_loss=5.64, eval_precision=0.75, eval_recall=0.765]\n",
      "Epoch: 6/30 Train: 100%|██████████| 98/98 [01:03<00:00,  1.55it/s, F1=0.934, train_acc=0.995, train_loss=0.488, train_precision=0.922, train_recall=0.947]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.09s/it, F1=0.742, eval_acc=0.963, eval_loss=6.52, eval_precision=0.756, eval_recall=0.733]\n",
      "Epoch: 7/30 Train: 100%|██████████| 98/98 [01:01<00:00,  1.59it/s, F1=0.944, train_acc=0.996, train_loss=0.446, train_precision=0.937, train_recall=0.953]\n",
      "Eval Result: 100%|██████████| 9/9 [00:31<00:00,  3.50s/it, F1=0.749, eval_acc=0.965, eval_loss=6.08, eval_precision=0.713, eval_recall=0.791]\n",
      "Epoch: 8/30 Train: 100%|██████████| 98/98 [01:04<00:00,  1.52it/s, F1=0.953, train_acc=0.996, train_loss=0.428, train_precision=0.947, train_recall=0.961]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.09s/it, F1=0.767, eval_acc=0.966, eval_loss=6.23, eval_precision=0.742, eval_recall=0.797]\n",
      "Epoch: 9/30 Train: 100%|██████████| 98/98 [01:04<00:00,  1.53it/s, F1=0.955, train_acc=0.995, train_loss=0.404, train_precision=0.95, train_recall=0.962]\n",
      "Eval Result: 100%|██████████| 9/9 [00:30<00:00,  3.39s/it, F1=0.764, eval_acc=0.966, eval_loss=7.62, eval_precision=0.77, eval_recall=0.759]\n",
      "Epoch: 10/30 Train: 100%|██████████| 98/98 [01:03<00:00,  1.55it/s, F1=0.968, train_acc=0.996, train_loss=0.313, train_precision=0.962, train_recall=0.975]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.09s/it, F1=0.764, eval_acc=0.965, eval_loss=6.85, eval_precision=0.776, eval_recall=0.755]\n",
      "Epoch: 11/30 Train: 100%|██████████| 98/98 [01:01<00:00,  1.59it/s, F1=0.979, train_acc=0.998, train_loss=0.238, train_precision=0.976, train_recall=0.983]\n",
      "Eval Result: 100%|██████████| 9/9 [00:29<00:00,  3.31s/it, F1=0.765, eval_acc=0.966, eval_loss=5.9, eval_precision=0.74, eval_recall=0.795]\n",
      "Epoch: 12/30 Train: 100%|██████████| 98/98 [01:06<00:00,  1.47it/s, F1=0.981, train_acc=0.998, train_loss=0.182, train_precision=0.981, train_recall=0.982]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.09s/it, F1=0.782, eval_acc=0.968, eval_loss=6.54, eval_precision=0.765, eval_recall=0.802]\n",
      "Epoch: 13/30 Train: 100%|██████████| 98/98 [01:01<00:00,  1.59it/s, F1=0.986, train_acc=0.999, train_loss=0.109, train_precision=0.984, train_recall=0.987]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.10s/it, F1=0.76, eval_acc=0.965, eval_loss=7.03, eval_precision=0.735, eval_recall=0.79]\n",
      "Epoch: 14/30 Train: 100%|██████████| 98/98 [01:01<00:00,  1.59it/s, F1=0.993, train_acc=0.999, train_loss=0.0826, train_precision=0.992, train_recall=0.994]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.10s/it, F1=0.78, eval_acc=0.968, eval_loss=6.41, eval_precision=0.758, eval_recall=0.805]\n",
      "Epoch: 15/30 Train: 100%|██████████| 98/98 [01:01<00:00,  1.59it/s, F1=0.992, train_acc=0.999, train_loss=0.0649, train_precision=0.992, train_recall=0.993]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.10s/it, F1=0.786, eval_acc=0.968, eval_loss=6.95, eval_precision=0.773, eval_recall=0.802]\n",
      "Epoch: 16/30 Train: 100%|██████████| 98/98 [01:01<00:00,  1.59it/s, F1=0.993, train_acc=0.999, train_loss=0.0729, train_precision=0.993, train_recall=0.994]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.08s/it, F1=0.773, eval_acc=0.966, eval_loss=7.39, eval_precision=0.749, eval_recall=0.802]\n",
      "Epoch: 17/30 Train: 100%|██████████| 98/98 [01:05<00:00,  1.50it/s, F1=0.998, train_acc=1, train_loss=0.0363, train_precision=0.998, train_recall=0.998]\n",
      "Eval Result: 100%|██████████| 9/9 [00:29<00:00,  3.30s/it, F1=0.782, eval_acc=0.968, eval_loss=7.14, eval_precision=0.759, eval_recall=0.808]\n",
      "Epoch: 18/30 Train: 100%|██████████| 98/98 [01:01<00:00,  1.59it/s, F1=0.996, train_acc=1, train_loss=0.0404, train_precision=0.996, train_recall=0.996]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.09s/it, F1=0.785, eval_acc=0.967, eval_loss=7.25, eval_precision=0.768, eval_recall=0.807]\n",
      "Epoch: 19/30 Train: 100%|██████████| 98/98 [01:02<00:00,  1.58it/s, F1=0.996, train_acc=1, train_loss=0.0399, train_precision=0.996, train_recall=0.997]\n",
      "Eval Result: 100%|██████████| 9/9 [00:29<00:00,  3.25s/it, F1=0.772, eval_acc=0.966, eval_loss=7.46, eval_precision=0.754, eval_recall=0.794]\n",
      "Epoch: 20/30 Train: 100%|██████████| 98/98 [01:03<00:00,  1.54it/s, F1=0.99, train_acc=0.999, train_loss=0.135, train_precision=0.989, train_recall=0.991]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.09s/it, F1=0.767, eval_acc=0.966, eval_loss=6.67, eval_precision=0.754, eval_recall=0.783]\n",
      "Epoch: 21/30 Train: 100%|██████████| 98/98 [01:05<00:00,  1.50it/s, F1=0.994, train_acc=1, train_loss=0.0796, train_precision=0.993, train_recall=0.995]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.09s/it, F1=0.78, eval_acc=0.966, eval_loss=7.31, eval_precision=0.772, eval_recall=0.791]\n",
      "Epoch: 22/30 Train: 100%|██████████| 98/98 [01:01<00:00,  1.58it/s, F1=0.996, train_acc=1, train_loss=0.0399, train_precision=0.995, train_recall=0.997]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.10s/it, F1=0.772, eval_acc=0.966, eval_loss=7.35, eval_precision=0.763, eval_recall=0.786]\n",
      "Epoch: 23/30 Train: 100%|██████████| 98/98 [01:01<00:00,  1.59it/s, F1=0.997, train_acc=1, train_loss=0.0359, train_precision=0.997, train_recall=0.997]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.10s/it, F1=0.78, eval_acc=0.967, eval_loss=7.04, eval_precision=0.76, eval_recall=0.806]\n",
      "Epoch: 24/30 Train: 100%|██████████| 98/98 [01:02<00:00,  1.56it/s, F1=0.996, train_acc=1, train_loss=0.035, train_precision=0.996, train_recall=0.997]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.09s/it, F1=0.776, eval_acc=0.967, eval_loss=7.28, eval_precision=0.754, eval_recall=0.804]\n",
      "Epoch: 25/30 Train: 100%|██████████| 98/98 [01:07<00:00,  1.46it/s, F1=0.998, train_acc=1, train_loss=0.0286, train_precision=0.998, train_recall=0.999]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.09s/it, F1=0.783, eval_acc=0.967, eval_loss=7.68, eval_precision=0.765, eval_recall=0.806]\n",
      "Epoch: 26/30 Train: 100%|██████████| 98/98 [01:05<00:00,  1.49it/s, F1=0.991, train_acc=0.999, train_loss=0.0937, train_precision=0.99, train_recall=0.992]\n",
      "Eval Result: 100%|██████████| 9/9 [00:31<00:00,  3.54s/it, F1=0.77, eval_acc=0.964, eval_loss=6.71, eval_precision=0.727, eval_recall=0.822]\n",
      "Epoch: 27/30 Train: 100%|██████████| 98/98 [01:05<00:00,  1.49it/s, F1=0.993, train_acc=0.999, train_loss=0.0925, train_precision=0.991, train_recall=0.994]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.08s/it, F1=0.772, eval_acc=0.966, eval_loss=7.23, eval_precision=0.739, eval_recall=0.812]\n",
      "Epoch: 28/30 Train: 100%|██████████| 98/98 [01:01<00:00,  1.59it/s, F1=0.994, train_acc=0.999, train_loss=0.0929, train_precision=0.994, train_recall=0.994]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.08s/it, F1=0.78, eval_acc=0.964, eval_loss=6.45, eval_precision=0.756, eval_recall=0.81]\n",
      "Epoch: 29/30 Train: 100%|██████████| 98/98 [01:01<00:00,  1.59it/s, F1=0.991, train_acc=0.999, train_loss=0.122, train_precision=0.99, train_recall=0.992]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.09s/it, F1=0.772, eval_acc=0.965, eval_loss=8.07, eval_precision=0.777, eval_recall=0.769]\n",
      "Epoch: 30/30 Train: 100%|██████████| 98/98 [01:06<00:00,  1.48it/s, F1=0.99, train_acc=0.999, train_loss=0.109, train_precision=0.991, train_recall=0.99]\n",
      "Eval Result: 100%|██████████| 9/9 [00:27<00:00,  3.09s/it, F1=0.78, eval_acc=0.967, eval_loss=6.6, eval_precision=0.768, eval_recall=0.795]\n"
     ]
    }
   ],
   "source": [
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super NER的Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from CC.predicter import NERPredict\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "# %%\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/Super_x_Ontonotes5/pre_train_0.2x.json',\n",
    "    'eval_file': './data/Super_x_Ontonotes5/pre_dev.json',\n",
    "    'test_file': './data/Super_x_Ontonotes5/pre_dev.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/Super_x_Ontonotes5/tags_list.txt',\n",
    "    'output_eval': True,\n",
    "    'loader_name': 'le_loader',\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 64,\n",
    "    'eval_batch_size': 512,\n",
    "    'do_shuffle': True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'super_x_ontonotes'\n",
    "}\n",
    "\n",
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2, resume_path='./save_model/super_x_ontonotes', resume_step=2586):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super NER的预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "import pickle\n",
    "from tqdm import *\n",
    "from CC.loaders.utils import *\n",
    "import json\n",
    "from CC.pre_trained import NERPreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1500000,\n",
    "    'train_file': './data/SuperNER/pre_train.json',\n",
    "    'eval_file': './data/SuperNER/pre_dev.json',\n",
    "    'test_file': './data/SuperNER/pre_test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/SuperNER/tags_list.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'Pre_trained',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"ORG\": \"机构\",\n",
    "        \"LOC\": \"地名\",\n",
    "        \"PER\": \"人名\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"度量\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weibo的预训练\n",
    "loader: lex_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "import pickle\n",
    "from tqdm import *\n",
    "from CC.loaders.utils import *\n",
    "import json\n",
    "from CC.pre_trained import NERPreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/weibo/train.json',\n",
    "    'eval_file': './data/weibo/dev.json',\n",
    "    'test_file': './data/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/weibo/pretrained_labels.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'Weibo_Full_Pretrained',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"PER.NOM\": \"指代人名\",\n",
    "        \"LOC.NAM\": \"地名\",\n",
    "        \"PER.NAM\": \"人名\",\n",
    "        \"GPE.NAM\": \"政体\",\n",
    "        \"ORG.NAM\": \"机构\",\n",
    "        \"ORG.NOM\": \"指代机构\",\n",
    "        \"LOC.NOM\": \"指代地名\",\n",
    "        \"GPE.NOM\": \"指代政体\",\n",
    "        \"ORG\": \"机构\",\n",
    "        \"LOC\": \"地名\",\n",
    "        \"PER\": \"人名\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"度量\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "import pickle\n",
    "from tqdm import *\n",
    "from CC.loaders.utils import *\n",
    "import json\n",
    "from CC.pre_trained import NERPreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/weibo/train.json',\n",
    "    'eval_file': './data/weibo/dev.json',\n",
    "    'test_file': './data/weibo/dev.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/weibo/pretrained_labels.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag_combine.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'Ontonotes5_02_Pretrained',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"PRODUCT\": \"产品\",\n",
    "        \"FAC\": \"场地\",\n",
    "        \"ORDINAL\": \"排名\",\n",
    "        \"QUANTITY\": \"无单位数量\",\n",
    "        \"CARDINAL\": \"有单位数量\",\n",
    "        \"EVENT\": \"事件\",\n",
    "        \"MONEY\": \"金额\",\n",
    "        \"DATE\": \"日期\",\n",
    "        \"LOC\": \"地名\",\n",
    "        \"WORK_OF_ART\": \"作品\",\n",
    "        \"NORP\": \"政体,民族或宗教\",\n",
    "        \"ORG\": \"机构\",\n",
    "        \"PERCENT\": \"百分数\",\n",
    "        \"LANGUAGE\": \"语言\",\n",
    "        \"GPE\": \"政体\",\n",
    "        \"PERSON\": \"人名\",\n",
    "        \"LAW\": \"法文\",\n",
    "        \"TIME\": \"时间\",\n",
    "        \"PER\": \"人名\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"度量\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "args['pretrained_file_name'] = './save_pretrained/Ontonotes5_003_Pretrained/Bert_1290/pytorch_model.bin'\n",
    "args['max_scan_num'] = '1000000'\n",
    "args['train_file'] = './data/ontonotes5/train_003.json'\n",
    "args['eval_file'] = './data/ontonotes5/dev.json'\n",
    "args['test_file'] = './data/ontonotes5/test.json'\n",
    "args['tag_file'] = './data/ontonotes5/ontonotes5_pretrained_labels.txt'\n",
    "args['task_name'] = 'Ontonotes5_003_Pretrained'\n",
    "\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer(lr=1e-5):\n",
    "    a = i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ontonote5(Four Labels)的预训练\n",
    "loader: lex_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "import pickle\n",
    "from tqdm import *\n",
    "from CC.loaders.utils import *\n",
    "import json\n",
    "from CC.pre_trained import NERPreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_epochs': 35,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/weibo/train.json',\n",
    "    'eval_file': './data/weibo/dev.json',\n",
    "    'test_file': './data/weibo/dev.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/weibo/pretrained_labels.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'pass_none_rule': True,\n",
    "    'skip_single_matched_word': True,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'Ontonotes5_02_Pretrained',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"LOC\": \"地名\",\n",
    "        \"NORP\": \"政体,民族或宗教\",\n",
    "        \"ORG\": \"机构\",\n",
    "        \"GPE\": \"政体\",\n",
    "        \"PERSON\": \"人名\",\n",
    "        \"PER\": \"人名\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"度量\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "args['pretrained_file_name'] = 'save_pretrained/Ontonotes5_s_02_Pretrained/Bert_1960/pytorch_model.bin'\n",
    "args['max_scan_num'] = '1000000'\n",
    "args['train_file'] = './data/ontonotes5_s/train_02_1.json'\n",
    "args['eval_file'] = './data/ontonotes5_s/dev.json'\n",
    "args['test_file'] = './data/ontonotes5_s/test.json'\n",
    "args['tag_file'] = './data/ontonotes5_s/pretrained_labels_ori.txt'\n",
    "args['task_name'] = 'Ontonotes5_s_02_Pretrained'\n",
    "\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer(lr=1e-5):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weibo的预训练(屏蔽无关实体)\n",
    "loader: lex_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "import pickle\n",
    "from tqdm import *\n",
    "from CC.loaders.utils import *\n",
    "import json\n",
    "from CC.pre_trained import NERPreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/weibo/train.json',\n",
    "    'eval_file': './data/weibo/dev.json',\n",
    "    'test_file': './data/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/weibo/pretrained_labels.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'Weibo_Full_Pretrained',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"pass_none_rule\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"PER.NOM\": \"指代人名\",\n",
    "        \"LOC.NAM\": \"地名\",\n",
    "        \"PER.NAM\": \"人名\",\n",
    "        \"GPE.NAM\": \"政体\",\n",
    "        \"ORG.NAM\": \"机构\",\n",
    "        \"ORG.NOM\": \"指代机构\",\n",
    "        \"LOC.NOM\": \"指代地名\",\n",
    "        \"GPE.NOM\": \"指代政体\",\n",
    "        \"ORG\": \"机构\",\n",
    "        \"LOC\": \"地名\",\n",
    "        \"PER\": \"人名\",\n",
    "        \"government\": \"政府\"\n",
    "    }\n",
    "}\n",
    "\n",
    "args['pretrained_file_name'] = './model/chinese_wwm_ext/pytorch_model.bin'\n",
    "args['max_scan_num'] = '1000000'\n",
    "args['train_file'] = './data/weibo/train.json'\n",
    "args['tag_file'] = './data/weibo/pretrained_labels.txt'\n",
    "args['task_name'] = 'Weibo_Full_Pretrained'\n",
    "\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 适用于`CONLL`的预训练\n",
    "loader: cnx_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "import pickle\n",
    "from tqdm import *\n",
    "from CC.loaders.utils import *\n",
    "import json\n",
    "from CC.pre_trained import NERPreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/weibonew/train.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/weibo/pretrained_labels.txt',\n",
    "    'loader_name': 'cnx_loader',\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'use_json': True,\n",
    "    'model_name': 'Bert',\n",
    "    'task_name': 'Weibo_CNX_Pretrained',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"PRODUCT\": \"产品\",\n",
    "        \"FAC\": \"场地\",\n",
    "        \"ORDINAL\": \"排名\",\n",
    "        \"QUANTITY\": \"无单位数量\",\n",
    "        \"CARDINAL\": \"有单位数量\",\n",
    "        \"EVENT\": \"事件\",\n",
    "        \"MONEY\": \"金额\",\n",
    "        \"DATE\": \"日期\",\n",
    "        \"LOC\": \"地名\",\n",
    "        \"WORK_OF_ART\": \"作品\",\n",
    "        \"NORP\": \"政体,民族或宗教\",\n",
    "        \"ORG\": \"机构\",\n",
    "        \"PERCENT\": \"百分数\",\n",
    "        \"LANGUAGE\": \"语言\",\n",
    "        \"GPE\": \"政体\",\n",
    "        \"PERSON\": \"人名\",\n",
    "        \"LAW\": \"法文\",\n",
    "        \"TIME\": \"时间\",\n",
    "        \"PER\": \"人名\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"度量\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "args['pretrained_file_name'] = './save_pretrained/Ontonotes5_02_Pretrained/Bert_2886/pytorch_model.bin'\n",
    "args['train_file'] = './data/ontonotes5/train_02_1.json'\n",
    "args['tag_file'] = './data/ontonotes5/ontonotes5_pretrained_labels.txt'\n",
    "args['task_name'] = 'Ontonotes5_02_Pretrained'\n",
    "\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer(lr=1e-5):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weibo 预训练x纠错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "import pickle\n",
    "from tqdm import *\n",
    "from CC.loaders.utils import *\n",
    "import json\n",
    "from CC.pre_trained import NERPreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/weibo/train_5x.json',\n",
    "    'eval_file': './data/weibo/dev.json',\n",
    "    'test_file': './data/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/weibo/pretrained_labels.txt',\n",
    "    'loader_name': 'labellex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'Weibo_Full20xC_Pretrained',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"PER.NOM\": \"指代人名\",\n",
    "        \"LOC.NAM\": \"地名\",\n",
    "        \"PER.NAM\": \"人名\",\n",
    "        \"GPE.NAM\": \"政体\",\n",
    "        \"ORG.NAM\": \"机构\",\n",
    "        \"ORG.NOM\": \"指代机构\",\n",
    "        \"LOC.NOM\": \"指代地名\",\n",
    "        \"GPE.NOM\": \"指代政体\",\n",
    "        \"ORG\": \"机构\",\n",
    "        \"LOC\": \"地名\",\n",
    "        \"PER\": \"人名\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"度量\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集扩展"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "\n",
    "loader = LabelLoader(**{\n",
    "    \"auto_loader\": False,\n",
    "    \"debug\": True,\n",
    "    \"file_name\": \"./data/weibo/train.json\",\n",
    "    \"random_rate\": 1.0,\n",
    "    \"expansion_rate\": 5\n",
    "}).read_data_set(\"./data/weibo/train.json\", 1.0) \\\n",
    "    .process_data(5) \\\n",
    "    .to_file(\"./train_5x.json\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e1e097b6c3c5a2a39328ddbc7de6327b7bd71c15618bc750f041eecacee4167"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pcpower': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
