{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from CC.predicter import NERPredict\n",
    "from CC.trainer import NERTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune 参数设置\n",
    "\n",
    "适用于**le_loader**下的fine-tune任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/weibo/train.json',\n",
    "    'eval_file': './data/weibo/dev.json',\n",
    "    'test_file': './data/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/weibo/labels.txt',\n",
    "    'output_eval': True,\n",
    "    'loader_name': 'le_loader',\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 16,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'weibo'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数微调\n",
    "\n",
    "调整`Pretrained`和`task name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['pretrained_file_name'] = './save_pretrained/Pre_trained/Bert_47640/pytorch_model.bin'\n",
    "args['train_file'] = './data/weibo/train.json'\n",
    "args['task_name'] = 'weibo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 16,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/weibo/train.json\",\n",
      "    \"eval_file\": \"./data/weibo/dev.json\",\n",
      "    \"test_file\": \"./data/weibo/test.json\",\n",
      "    \"tag_file\": \"./data/weibo/labels.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 150,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"weibo\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load vocabs into trie: 100%|██████████| 1000000/1000000 [00:01<00:00, 658781.44it/s]\n",
      "build trie: 100%|██████████| 1000000/1000000 [00:09<00:00, 102501.82it/s]\n",
      "load dataset matched word: 100%|██████████| 1351/1351 [00:00<00:00, 4648.53it/s]\n",
      "load dataset matched word: 100%|██████████| 271/271 [00:00<00:00, 4812.45it/s]\n",
      "load dataset matched word: 100%|██████████| 271/271 [00:00<00:00, 4681.66it/s]\n",
      "load vocab from list: 100%|██████████| 28457/28457 [00:00<00:00, 743886.35it/s]\n",
      "load vocab from files: 100%|██████████| 28/28 [00:00<00:00, 87057.46it/s]\n",
      "load vocab from list: 100%|██████████| 28/28 [00:00<00:00, 287844.39it/s]\n",
      "/home/lpc/miniconda3/envs/pcpower/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1618: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "load dataset from ./data/weibo/train.json: 100%|██████████| 1351/1351 [00:00<00:00, 1474.91it/s]\n",
      "load dataset from ./data/weibo/dev.json: 100%|██████████| 271/271 [00:00<00:00, 1489.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./save_pretrained/Pre_trained/Bert_47640/pytorch_model.bin were not used when initializing LEBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing LEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LEBertModel were not initialized from the model checkpoint at ./save_pretrained/Pre_trained/Bert_47640/pytorch_model.bin and are newly initialized: ['word_embeddings.weight', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.word_transform.bias', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train: 100%|██████████| 85/85 [01:22<00:00,  1.03it/s, F1=0.000409, train_acc=0.834, train_loss=40.8, train_precision=0.000253, train_recall=0.00106]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s, F1=0.0383, eval_acc=0.935, eval_loss=13.3, eval_precision=0.276, eval_recall=0.0206]\n",
      "Epoch: 2/30 Train: 100%|██████████| 85/85 [01:16<00:00,  1.11it/s, F1=0.305, train_acc=0.947, train_loss=11.6, train_precision=0.406, train_recall=0.244]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s, F1=0.504, eval_acc=0.957, eval_loss=6.89, eval_precision=0.558, eval_recall=0.46]\n",
      "Epoch: 3/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.17it/s, F1=0.542, train_acc=0.961, train_loss=6.53, train_precision=0.572, train_recall=0.516]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.70it/s, F1=0.575, eval_acc=0.96, eval_loss=6.17, eval_precision=0.571, eval_recall=0.578]\n",
      "Epoch: 4/30 Train: 100%|██████████| 85/85 [01:11<00:00,  1.18it/s, F1=0.677, train_acc=0.972, train_loss=4.15, train_precision=0.7, train_recall=0.655]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.72it/s, F1=0.603, eval_acc=0.96, eval_loss=5.92, eval_precision=0.739, eval_recall=0.509]\n",
      "Epoch: 5/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.17it/s, F1=0.771, train_acc=0.978, train_loss=2.98, train_precision=0.786, train_recall=0.755]\n",
      "Eval Result: 100%|██████████| 5/5 [00:03<00:00,  1.64it/s, F1=0.658, eval_acc=0.965, eval_loss=4.94, eval_precision=0.697, eval_recall=0.622]\n",
      "Epoch: 6/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.17it/s, F1=0.83, train_acc=0.985, train_loss=2.16, train_precision=0.844, train_recall=0.816]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s, F1=0.666, eval_acc=0.967, eval_loss=4.71, eval_precision=0.709, eval_recall=0.627]\n",
      "Epoch: 7/30 Train: 100%|██████████| 85/85 [01:16<00:00,  1.11it/s, F1=0.875, train_acc=0.988, train_loss=1.64, train_precision=0.884, train_recall=0.866]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.68it/s, F1=0.668, eval_acc=0.966, eval_loss=4.62, eval_precision=0.707, eval_recall=0.632]\n",
      "Epoch: 8/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.18it/s, F1=0.897, train_acc=0.99, train_loss=1.33, train_precision=0.901, train_recall=0.894]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s, F1=0.654, eval_acc=0.961, eval_loss=4.76, eval_precision=0.657, eval_recall=0.65]\n",
      "Epoch: 9/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.17it/s, F1=0.915, train_acc=0.992, train_loss=1.1, train_precision=0.922, train_recall=0.909]\n",
      "Eval Result: 100%|██████████| 5/5 [00:03<00:00,  1.56it/s, F1=0.667, eval_acc=0.965, eval_loss=5.15, eval_precision=0.725, eval_recall=0.617]\n",
      "Epoch: 10/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.18it/s, F1=0.935, train_acc=0.994, train_loss=0.932, train_precision=0.939, train_recall=0.932]\n",
      "Eval Result: 100%|██████████| 5/5 [00:03<00:00,  1.58it/s, F1=0.647, eval_acc=0.964, eval_loss=5.67, eval_precision=0.686, eval_recall=0.612]\n",
      "Epoch: 11/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.18it/s, F1=0.953, train_acc=0.996, train_loss=0.732, train_precision=0.955, train_recall=0.952]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.72it/s, F1=0.659, eval_acc=0.963, eval_loss=5.44, eval_precision=0.652, eval_recall=0.666]\n",
      "Epoch: 12/30 Train: 100%|██████████| 85/85 [01:16<00:00,  1.11it/s, F1=0.956, train_acc=0.996, train_loss=0.69, train_precision=0.958, train_recall=0.955]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.70it/s, F1=0.66, eval_acc=0.962, eval_loss=5.91, eval_precision=0.614, eval_recall=0.715]\n",
      "Epoch: 13/30 Train: 100%|██████████| 85/85 [01:11<00:00,  1.19it/s, F1=0.961, train_acc=0.996, train_loss=0.584, train_precision=0.962, train_recall=0.96]\n",
      "Eval Result: 100%|██████████| 5/5 [00:03<00:00,  1.46it/s, F1=0.66, eval_acc=0.964, eval_loss=5.62, eval_precision=0.64, eval_recall=0.681]\n",
      "Epoch: 14/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.18it/s, F1=0.97, train_acc=0.997, train_loss=0.525, train_precision=0.967, train_recall=0.972]\n",
      "Eval Result: 100%|██████████| 5/5 [00:03<00:00,  1.51it/s, F1=0.668, eval_acc=0.964, eval_loss=6.03, eval_precision=0.69, eval_recall=0.648]\n",
      "Epoch: 15/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.17it/s, F1=0.971, train_acc=0.997, train_loss=0.466, train_precision=0.972, train_recall=0.97]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s, F1=0.663, eval_acc=0.964, eval_loss=5.85, eval_precision=0.663, eval_recall=0.663]\n",
      "Epoch: 16/30 Train: 100%|██████████| 85/85 [01:11<00:00,  1.18it/s, F1=0.973, train_acc=0.997, train_loss=0.442, train_precision=0.975, train_recall=0.972]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s, F1=0.663, eval_acc=0.963, eval_loss=6.01, eval_precision=0.632, eval_recall=0.697]\n",
      "Epoch: 17/30 Train: 100%|██████████| 85/85 [01:17<00:00,  1.10it/s, F1=0.974, train_acc=0.997, train_loss=0.414, train_precision=0.975, train_recall=0.972]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.72it/s, F1=0.648, eval_acc=0.961, eval_loss=6.76, eval_precision=0.608, eval_recall=0.694]\n",
      "Epoch: 18/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.17it/s, F1=0.969, train_acc=0.997, train_loss=0.478, train_precision=0.969, train_recall=0.969]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s, F1=0.678, eval_acc=0.965, eval_loss=6.19, eval_precision=0.647, eval_recall=0.712]\n",
      "Epoch: 19/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.17it/s, F1=0.976, train_acc=0.997, train_loss=0.363, train_precision=0.976, train_recall=0.976]\n",
      "Eval Result: 100%|██████████| 5/5 [00:03<00:00,  1.57it/s, F1=0.667, eval_acc=0.963, eval_loss=6.39, eval_precision=0.633, eval_recall=0.704]\n",
      "Epoch: 20/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.17it/s, F1=0.976, train_acc=0.998, train_loss=0.385, train_precision=0.973, train_recall=0.978]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.72it/s, F1=0.661, eval_acc=0.963, eval_loss=6.61, eval_precision=0.623, eval_recall=0.704]\n",
      "Epoch: 21/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.17it/s, F1=0.974, train_acc=0.998, train_loss=0.392, train_precision=0.972, train_recall=0.976]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s, F1=0.671, eval_acc=0.963, eval_loss=6.59, eval_precision=0.632, eval_recall=0.715]\n",
      "Epoch: 22/30 Train: 100%|██████████| 85/85 [01:16<00:00,  1.11it/s, F1=0.977, train_acc=0.998, train_loss=0.362, train_precision=0.975, train_recall=0.979]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s, F1=0.656, eval_acc=0.963, eval_loss=6.39, eval_precision=0.646, eval_recall=0.666]\n",
      "Epoch: 23/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.17it/s, F1=0.971, train_acc=0.997, train_loss=0.407, train_precision=0.971, train_recall=0.971]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s, F1=0.655, eval_acc=0.964, eval_loss=5.95, eval_precision=0.644, eval_recall=0.666]\n",
      "Epoch: 24/30 Train: 100%|██████████| 85/85 [01:11<00:00,  1.18it/s, F1=0.976, train_acc=0.998, train_loss=0.391, train_precision=0.976, train_recall=0.977]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.72it/s, F1=0.662, eval_acc=0.963, eval_loss=5.8, eval_precision=0.65, eval_recall=0.674]\n",
      "Epoch: 25/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.18it/s, F1=0.978, train_acc=0.997, train_loss=0.347, train_precision=0.978, train_recall=0.977]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.72it/s, F1=0.667, eval_acc=0.965, eval_loss=6.08, eval_precision=0.678, eval_recall=0.656]\n",
      "Epoch: 26/30 Train: 100%|██████████| 85/85 [01:17<00:00,  1.10it/s, F1=0.982, train_acc=0.998, train_loss=0.293, train_precision=0.981, train_recall=0.983]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.72it/s, F1=0.662, eval_acc=0.964, eval_loss=6.82, eval_precision=0.639, eval_recall=0.686]\n",
      "Epoch: 27/30 Train: 100%|██████████| 85/85 [01:11<00:00,  1.18it/s, F1=0.978, train_acc=0.998, train_loss=0.32, train_precision=0.978, train_recall=0.978]\n",
      "Eval Result: 100%|██████████| 5/5 [00:03<00:00,  1.55it/s, F1=0.665, eval_acc=0.965, eval_loss=7.35, eval_precision=0.677, eval_recall=0.653]\n",
      "Epoch: 28/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.17it/s, F1=0.981, train_acc=0.998, train_loss=0.288, train_precision=0.98, train_recall=0.982]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.72it/s, F1=0.659, eval_acc=0.963, eval_loss=6.24, eval_precision=0.636, eval_recall=0.684]\n",
      "Epoch: 29/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.17it/s, F1=0.979, train_acc=0.998, train_loss=0.341, train_precision=0.979, train_recall=0.978]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s, F1=0.663, eval_acc=0.965, eval_loss=7.02, eval_precision=0.661, eval_recall=0.666]\n",
      "Epoch: 30/30 Train: 100%|██████████| 85/85 [01:12<00:00,  1.17it/s, F1=0.979, train_acc=0.998, train_loss=0.292, train_precision=0.98, train_recall=0.978]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s, F1=0.652, eval_acc=0.963, eval_loss=6.57, eval_precision=0.625, eval_recall=0.681]\n"
     ]
    }
   ],
   "source": [
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer(lr2=1e-2):\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super NER的预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "import pickle\n",
    "from tqdm import *\n",
    "from CC.loaders.utils import *\n",
    "import json\n",
    "from CC.pre_trained import NERPreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1500000,\n",
    "    'train_file': './data/SuperNER/pre_train.json',\n",
    "    'eval_file': './data/SuperNER/pre_dev.json',\n",
    "    'test_file': './data/SuperNER/pre_test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/SuperNER/tags_list.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'Pre_trained',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"ORG\": \"组织\",\n",
    "        \"LOC\": \"地点\",\n",
    "        \"PER\": \"人\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"测量单位\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weibo的预训练\n",
    "loader: lex_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "import pickle\n",
    "from tqdm import *\n",
    "from CC.loaders.utils import *\n",
    "import json\n",
    "from CC.pre_trained import NERPreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/weibo/train.json',\n",
    "    'eval_file': './data/weibo/dev.json',\n",
    "    'test_file': './data/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/weibo/pretrained_labels.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'Weibo_Full_Pretrained',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"PER.NOM\": \"人的象征\",\n",
    "        \"LOC.NAM\": \"地点\",\n",
    "        \"PER.NAM\": \"人\",\n",
    "        \"GPE.NAM\": \"政治实体\",\n",
    "        \"ORG.NAM\": \"组织\",\n",
    "        \"ORG.NOM\": \"组织的象征\",\n",
    "        \"LOC.NOM\": \"地点的象征\",\n",
    "        \"GPE.NOM\": \"政治实体的象征\",\n",
    "        \"ORG\": \"组织\",\n",
    "        \"LOC\": \"地点\",\n",
    "        \"PER\": \"人\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"测量单位\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 适用于`CONLL`的预训练\n",
    "loader: cnx_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "import pickle\n",
    "from tqdm import *\n",
    "from CC.loaders.utils import *\n",
    "import json\n",
    "from CC.pre_trained import NERPreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/weibonew/train.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/weibo/pretrained_labels.txt',\n",
    "    'loader_name': 'cnx_loader',\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'use_json': True,\n",
    "    'model_name': 'Bert',\n",
    "    'task_name': 'Weibo_CNX_Pretrained',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"PER.NOM\": \"人的象征\",\n",
    "        \"LOC.NAM\": \"地点\",\n",
    "        \"PER.NAM\": \"人\",\n",
    "        \"GPE.NAM\": \"政治实体\",\n",
    "        \"ORG.NAM\": \"组织\",\n",
    "        \"ORG.NOM\": \"组织的象征\",\n",
    "        \"LOC.NOM\": \"地点的象征\",\n",
    "        \"GPE.NOM\": \"政治实体的象征\",\n",
    "        \"ORG\": \"组织\",\n",
    "        \"LOC\": \"地点\",\n",
    "        \"PER\": \"人\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"测量单位\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集扩展"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "\n",
    "loader = LabelLoader(**{\n",
    "    \"auto_loader\": False,\n",
    "    \"debug\": True,\n",
    "    \"file_name\": \"./data/weibo/train.json\",\n",
    "    \"random_rate\": 1.0,\n",
    "    \"expansion_rate\": 20\n",
    "}).read_data_set(\"./data/weibo/train.json\", 1.0) \\\n",
    "    .process_data(20) \\\n",
    "    .to_file(\"./train_20x.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e1e097b6c3c5a2a39328ddbc7de6327b7bd71c15618bc750f041eecacee4167"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pcpower': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
