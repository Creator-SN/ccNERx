{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from CC.predicter import NERPredict\n",
    "from CC.trainer import NERTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune 参数设置\n",
    "\n",
    "适用于**le_loader**下的fine-tune任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'hidden_dim': 300,\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/weibo/train.json',\n",
    "    'eval_file': './data/weibo/dev.json',\n",
    "    'test_file': './data/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/weibo/labels.txt',\n",
    "    'output_eval': True,\n",
    "    'loader_name': 'le_loader',\n",
    "    \"word_embedding_file\":\"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\":\"./data/tencent/tencent_vocab.txt\",\n",
    "    \"default_tag\":\"O\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'model_name': 'LEBert',\n",
    "    'task_name': 'weibo'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数微调\n",
    "\n",
    "调整`Pretrained`和`task name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['pretrained_file_name'] = './save_pretrained/Weibo_x20_Pretrained/Bert_2024/pytorch_model.bin'\n",
    "args['train_file'] = './data/weibo_yfy/train_origin.json'\n",
    "args['task_name'] = 'weibo_new_02_yfy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 8,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"train_file\": \"./data/weibo_yfy/train_origin.json\",\n",
      "    \"eval_file\": \"./data/weibo/dev.json\",\n",
      "    \"test_file\": \"./data/weibo/test.json\",\n",
      "    \"tag_file\": \"./data/weibo/labels.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 150,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"task_name\": \"weibo_new_02_yfy\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load vocabs into trie: 100%|██████████| 1000000/1000000 [00:01<00:00, 657144.66it/s]\n",
      "build trie: 100%|██████████| 1000000/1000000 [00:09<00:00, 105574.83it/s]\n",
      "load dataset matched word: 100%|██████████| 270/270 [00:00<00:00, 4667.14it/s]\n",
      "load dataset matched word: 100%|██████████| 271/271 [00:00<00:00, 4855.08it/s]\n",
      "load dataset matched word: 100%|██████████| 271/271 [00:00<00:00, 4749.27it/s]\n",
      "load vocab from list: 100%|██████████| 16453/16453 [00:00<00:00, 726845.41it/s]\n",
      "load vocab from files: 100%|██████████| 28/28 [00:00<00:00, 93206.76it/s]\n",
      "load vocab from list: 100%|██████████| 28/28 [00:00<00:00, 285049.79it/s]\n",
      "/home/lpc/miniconda3/envs/pcpower/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1618: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "load dataset from ./data/weibo_yfy/train_origin.json: 100%|██████████| 270/270 [00:00<00:00, 1379.70it/s]\n",
      "load dataset from ./data/weibo/dev.json: 100%|██████████| 271/271 [00:00<00:00, 1389.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained embedding from file.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./save_pretrained/Weibo_x20_Pretrained/Bert_2024/pytorch_model.bin were not used when initializing LEBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing LEBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LEBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LEBertModel were not initialized from the model checkpoint at ./save_pretrained/Weibo_x20_Pretrained/Bert_2024/pytorch_model.bin and are newly initialized: ['word_embeddings.weight', 'bert.encoder.layer.0.attn_W', 'bert.encoder.layer.0.word_transform.weight', 'bert.encoder.layer.0.word_transform.bias', 'bert.encoder.layer.0.word_word_weight.weight', 'bert.encoder.layer.0.word_word_weight.bias', 'bert.encoder.layer.0.fuse_layernorm.weight', 'bert.encoder.layer.0.fuse_layernorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch: 1/30 Train: 100%|██████████| 34/34 [00:26<00:00,  1.29it/s, F1=0.000355, train_acc=0.17, train_loss=156, train_precision=0.000184, train_recall=0.00489]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/lpc/miniconda3/envs/pcpower/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0, eval_acc=0.933, eval_loss=48.2, eval_precision=0, eval_recall=0]\n",
      "Epoch: 2/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.08it/s, F1=0, train_acc=0.927, train_loss=29.6, train_precision=0, train_recall=0]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.73it/s, F1=0, eval_acc=0.933, eval_loss=21.7, eval_precision=0, eval_recall=0]\n",
      "Epoch: 3/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.05it/s, F1=0, train_acc=0.927, train_loss=24.8, train_precision=0, train_recall=0]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0, eval_acc=0.933, eval_loss=20.3, eval_precision=0, eval_recall=0]\n",
      "Epoch: 4/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.09it/s, F1=0, train_acc=0.927, train_loss=23.6, train_precision=0, train_recall=0]\n",
      "Eval Result: 100%|██████████| 5/5 [00:07<00:00,  1.45s/it, F1=0, eval_acc=0.933, eval_loss=19, eval_precision=0, eval_recall=0]\n",
      "Epoch: 5/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.09it/s, F1=0, train_acc=0.927, train_loss=20.2, train_precision=0, train_recall=0]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.75it/s, F1=0, eval_acc=0.933, eval_loss=12.8, eval_precision=0, eval_recall=0]\n",
      "Epoch: 6/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.06it/s, F1=0.193, train_acc=0.939, train_loss=13.4, train_precision=0.206, train_recall=0.181]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.372, eval_acc=0.951, eval_loss=9.23, eval_precision=0.494, eval_recall=0.298]\n",
      "Epoch: 7/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.08it/s, F1=0.34, train_acc=0.958, train_loss=9.11, train_precision=0.314, train_recall=0.372]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.444, eval_acc=0.959, eval_loss=7.73, eval_precision=0.367, eval_recall=0.56]\n",
      "Epoch: 8/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.08it/s, F1=0.473, train_acc=0.971, train_loss=6.31, train_precision=0.413, train_recall=0.555]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.425, eval_acc=0.953, eval_loss=7.69, eval_precision=0.328, eval_recall=0.604]\n",
      "Epoch: 9/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.08it/s, F1=0.6, train_acc=0.977, train_loss=4.78, train_precision=0.552, train_recall=0.658]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.545, eval_acc=0.964, eval_loss=6.34, eval_precision=0.49, eval_recall=0.614]\n",
      "Epoch: 10/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.08it/s, F1=0.664, train_acc=0.98, train_loss=3.99, train_precision=0.611, train_recall=0.726]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.592, eval_acc=0.964, eval_loss=7.12, eval_precision=0.668, eval_recall=0.532]\n",
      "Epoch: 11/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.07it/s, F1=0.747, train_acc=0.982, train_loss=4.09, train_precision=0.733, train_recall=0.76]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.72it/s, F1=0.613, eval_acc=0.962, eval_loss=6.77, eval_precision=0.543, eval_recall=0.704]\n",
      "Epoch: 12/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.08it/s, F1=0.819, train_acc=0.991, train_loss=2.33, train_precision=0.788, train_recall=0.853]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.625, eval_acc=0.968, eval_loss=6.38, eval_precision=0.587, eval_recall=0.668]\n",
      "Epoch: 13/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.09it/s, F1=0.861, train_acc=0.994, train_loss=1.62, train_precision=0.84, train_recall=0.883]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.639, eval_acc=0.967, eval_loss=6.56, eval_precision=0.589, eval_recall=0.699]\n",
      "Epoch: 14/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.04it/s, F1=0.907, train_acc=0.995, train_loss=1.22, train_precision=0.897, train_recall=0.917]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.66, eval_acc=0.967, eval_loss=6.38, eval_precision=0.612, eval_recall=0.715]\n",
      "Epoch: 15/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.04it/s, F1=0.931, train_acc=0.997, train_loss=0.942, train_precision=0.917, train_recall=0.946]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.668, eval_acc=0.969, eval_loss=6.51, eval_precision=0.622, eval_recall=0.722]\n",
      "Epoch: 16/30 Train: 100%|██████████| 34/34 [00:21<00:00,  1.61it/s, F1=0.94, train_acc=0.998, train_loss=0.729, train_precision=0.926, train_recall=0.954]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.664, eval_acc=0.969, eval_loss=6.72, eval_precision=0.614, eval_recall=0.722]\n",
      "Epoch: 17/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.06it/s, F1=0.955, train_acc=0.998, train_loss=0.633, train_precision=0.945, train_recall=0.966]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.75it/s, F1=0.664, eval_acc=0.969, eval_loss=6.72, eval_precision=0.609, eval_recall=0.73]\n",
      "Epoch: 18/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.05it/s, F1=0.953, train_acc=0.998, train_loss=0.516, train_precision=0.943, train_recall=0.963]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s, F1=0.662, eval_acc=0.968, eval_loss=6.85, eval_precision=0.616, eval_recall=0.715]\n",
      "Epoch: 19/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.05it/s, F1=0.96, train_acc=0.998, train_loss=0.441, train_precision=0.954, train_recall=0.966]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.663, eval_acc=0.969, eval_loss=6.97, eval_precision=0.612, eval_recall=0.722]\n",
      "Epoch: 20/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.04it/s, F1=0.968, train_acc=0.999, train_loss=0.415, train_precision=0.966, train_recall=0.971]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.73it/s, F1=0.67, eval_acc=0.968, eval_loss=7.18, eval_precision=0.603, eval_recall=0.753]\n",
      "Epoch: 21/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.07it/s, F1=0.971, train_acc=0.999, train_loss=0.343, train_precision=0.962, train_recall=0.98]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.672, eval_acc=0.969, eval_loss=7.29, eval_precision=0.631, eval_recall=0.72]\n",
      "Epoch: 22/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.07it/s, F1=0.972, train_acc=0.999, train_loss=0.362, train_precision=0.962, train_recall=0.983]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.661, eval_acc=0.967, eval_loss=7.08, eval_precision=0.606, eval_recall=0.728]\n",
      "Epoch: 23/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.06it/s, F1=0.976, train_acc=0.999, train_loss=0.272, train_precision=0.971, train_recall=0.98]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.668, eval_acc=0.968, eval_loss=7.55, eval_precision=0.636, eval_recall=0.704]\n",
      "Epoch: 24/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.05it/s, F1=0.979, train_acc=0.999, train_loss=0.227, train_precision=0.973, train_recall=0.985]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.73it/s, F1=0.671, eval_acc=0.968, eval_loss=7.6, eval_precision=0.621, eval_recall=0.73]\n",
      "Epoch: 25/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.03it/s, F1=0.978, train_acc=0.999, train_loss=0.255, train_precision=0.973, train_recall=0.983]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.73it/s, F1=0.663, eval_acc=0.968, eval_loss=7.66, eval_precision=0.629, eval_recall=0.702]\n",
      "Epoch: 26/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.05it/s, F1=0.968, train_acc=0.997, train_loss=0.476, train_precision=0.961, train_recall=0.976]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.70it/s, F1=0.646, eval_acc=0.965, eval_loss=7.58, eval_precision=0.577, eval_recall=0.735]\n",
      "Epoch: 27/30 Train: 100%|██████████| 34/34 [00:21<00:00,  1.60it/s, F1=0.981, train_acc=0.999, train_loss=0.288, train_precision=0.973, train_recall=0.988]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.643, eval_acc=0.967, eval_loss=9.64, eval_precision=0.687, eval_recall=0.604]\n",
      "Epoch: 28/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.06it/s, F1=0.961, train_acc=0.996, train_loss=1.08, train_precision=0.963, train_recall=0.958]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.682, eval_acc=0.969, eval_loss=7, eval_precision=0.644, eval_recall=0.725]\n",
      "Epoch: 29/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.02it/s, F1=0.978, train_acc=0.998, train_loss=0.469, train_precision=0.971, train_recall=0.985]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.707, eval_acc=0.972, eval_loss=7.3, eval_precision=0.675, eval_recall=0.743]\n",
      "Epoch: 30/30 Train: 100%|██████████| 34/34 [00:16<00:00,  2.07it/s, F1=0.979, train_acc=0.998, train_loss=0.319, train_precision=0.978, train_recall=0.98]\n",
      "Eval Result: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s, F1=0.68, eval_acc=0.97, eval_loss=7.65, eval_precision=0.652, eval_recall=0.71]\n"
     ]
    }
   ],
   "source": [
    "trainer = NERTrainer(**args)\n",
    "\n",
    "for i in trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super NER的预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "import pickle\n",
    "from tqdm import *\n",
    "from CC.loaders.utils import *\n",
    "import json\n",
    "from CC.pre_trained import NERPreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1500000,\n",
    "    'train_file': './data/SuperNER/pre_train.json',\n",
    "    'eval_file': './data/SuperNER/pre_dev.json',\n",
    "    'test_file': './data/SuperNER/pre_test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/SuperNER/tags_list.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'Pre_trained',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"ORG\": \"组织\",\n",
    "        \"LOC\": \"地点\",\n",
    "        \"PER\": \"人\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"测量单位\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weibo的预训练\n",
    "loader: lex_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "import pickle\n",
    "from tqdm import *\n",
    "from CC.loaders.utils import *\n",
    "import json\n",
    "from CC.pre_trained import NERPreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs parser: {\n",
      "    \"batch_size\": 32,\n",
      "    \"eval_batch_size\": 64,\n",
      "    \"test_batch_size\": 16,\n",
      "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
      "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
      "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
      "    \"train_file\": \"./data/weibonew/train.json\",\n",
      "    \"eval_file\": \"./data/weibo/dev.json\",\n",
      "    \"test_file\": \"./data/weibo/test.json\",\n",
      "    \"tag_file\": \"./data/weibo/pretrained_labels.txt\",\n",
      "    \"bert_vocab_file\": \"./model/chinese_wwm_ext/vocab.txt\",\n",
      "    \"output_eval\": true,\n",
      "    \"max_scan_num\": 1000000,\n",
      "    \"add_seq_vocab\": false,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_word_num\": 5,\n",
      "    \"default_tag\": \"O\",\n",
      "    \"use_test\": false,\n",
      "    \"do_shuffle\": true,\n",
      "    \"do_predict\": false,\n",
      "    \"lexicon_tree_cache_path\": null,\n",
      "    \"word_vacab_cache_path\": null,\n",
      "    \"task_name\": \"Weibo_x20_Pretrained\",\n",
      "    \"tag_rules\": {\n",
      "        \"PER.NOM\": \"人的象征\",\n",
      "        \"LOC.NAM\": \"地点\",\n",
      "        \"PER.NAM\": \"人\",\n",
      "        \"GPE.NAM\": \"政治实体\",\n",
      "        \"ORG.NAM\": \"组织\",\n",
      "        \"ORG.NOM\": \"组织的象征\",\n",
      "        \"LOC.NOM\": \"地点的象征\",\n",
      "        \"GPE.NOM\": \"政治实体的象征\",\n",
      "        \"ORG\": \"组织\",\n",
      "        \"LOC\": \"地点\",\n",
      "        \"PER\": \"人\",\n",
      "        \"Time\": \"时间\",\n",
      "        \"Thing\": \"物品\",\n",
      "        \"Metric\": \"测量单位\",\n",
      "        \"Abstract\": \"作品\",\n",
      "        \"Physical\": \"实体\",\n",
      "        \"Term\": \"术语\",\n",
      "        \"company\": \"企业\",\n",
      "        \"name\": \"名字\",\n",
      "        \"game\": \"游戏\",\n",
      "        \"movie\": \"电影\",\n",
      "        \"position\": \"职位\",\n",
      "        \"address\": \"地址\",\n",
      "        \"government\": \"政府\",\n",
      "        \"scene\": \"景点\",\n",
      "        \"book\": \"书名\"\n",
      "    },\n",
      "    \"debug\": true\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load vocabs into trie: 100%|██████████| 1000000/1000000 [00:01<00:00, 657181.11it/s]\n",
      "build trie: 100%|██████████| 1000000/1000000 [00:09<00:00, 101629.29it/s]\n",
      "load dataset matched word: 100%|██████████| 2915/2915 [00:00<00:00, 4267.35it/s]\n",
      "load dataset matched word: 100%|██████████| 271/271 [00:00<00:00, 4726.42it/s]\n",
      "load dataset matched word: 100%|██████████| 271/271 [00:00<00:00, 4699.84it/s]\n",
      "load vocab from files: 100%|██████████| 1000000/1000000 [00:07<00:00, 142470.87it/s]\n",
      "load vocab from list: 100%|██████████| 1000000/1000000 [00:02<00:00, 371387.72it/s]\n",
      "load vocab from list: 100%|██████████| 17670/17670 [00:00<00:00, 437610.94it/s]\n",
      "load vocab from files: 100%|██████████| 65/65 [00:00<00:00, 258172.12it/s]\n",
      "load vocab from list: 100%|██████████| 65/65 [00:00<00:00, 416865.08it/s]\n",
      "/home/lpc/miniconda3/envs/pcpower/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1618: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "load dataset from ./data/weibonew/train.json: 100%|██████████| 2915/2915 [00:02<00:00, 1004.80it/s]\n",
      "load dataset from ./data/weibo/dev.json: 100%|██████████| 271/271 [00:00<00:00, 1180.15it/s]\n",
      "Some weights of the model checkpoint at ./model/chinese_wwm_ext/pytorch_model.bin were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/92 [00:00<?, ?it/s]/home/lpc/miniconda3/envs/pcpower/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Epoch: 1/30 Train: 100%|██████████| 92/92 [01:18<00:00,  1.17it/s, train_loss=0.474]\n",
      "Epoch: 2/30 Train: 100%|██████████| 92/92 [01:05<00:00,  1.40it/s, train_loss=0.0283]\n",
      "Epoch: 3/30 Train: 100%|██████████| 92/92 [01:05<00:00,  1.39it/s, train_loss=0.00992]\n",
      "Epoch: 4/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.00422]\n",
      "Epoch: 5/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.00398]\n",
      "Epoch: 6/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.00199]\n",
      "Epoch: 7/30 Train: 100%|██████████| 92/92 [01:10<00:00,  1.30it/s, train_loss=0.00167]\n",
      "Epoch: 8/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.00119]\n",
      "Epoch: 9/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.00141]\n",
      "Epoch: 10/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.00115]\n",
      "Epoch: 11/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.00182]\n",
      "Epoch: 12/30 Train: 100%|██████████| 92/92 [01:10<00:00,  1.30it/s, train_loss=0.00189]\n",
      "Epoch: 13/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.00113]\n",
      "Epoch: 14/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.000981]\n",
      "Epoch: 15/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.00146]\n",
      "Epoch: 16/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.00129]\n",
      "Epoch: 17/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.00393]\n",
      "Epoch: 18/30 Train: 100%|██████████| 92/92 [01:10<00:00,  1.30it/s, train_loss=0.00194]\n",
      "Epoch: 19/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.000703]\n",
      "Epoch: 20/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.000904]\n",
      "Epoch: 21/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.000765]\n",
      "Epoch: 22/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.000614]\n",
      "Epoch: 23/30 Train: 100%|██████████| 92/92 [01:10<00:00,  1.30it/s, train_loss=0.000827]\n",
      "Epoch: 24/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.000815]\n",
      "Epoch: 25/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.00374]\n",
      "Epoch: 26/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.0031]\n",
      "Epoch: 27/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.00213]\n",
      "Epoch: 28/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.00276]\n",
      "Epoch: 29/30 Train: 100%|██████████| 92/92 [01:10<00:00,  1.30it/s, train_loss=0.00179]\n",
      "Epoch: 30/30 Train: 100%|██████████| 92/92 [01:06<00:00,  1.39it/s, train_loss=0.00178]\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/weibonew/train.json',\n",
    "    'eval_file': './data/weibo/dev.json',\n",
    "    'test_file': './data/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/weibo/pretrained_labels.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'Weibo_x20_Pretrained',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"PER.NOM\": \"人的象征\",\n",
    "        \"LOC.NAM\": \"地点\",\n",
    "        \"PER.NAM\": \"人\",\n",
    "        \"GPE.NAM\": \"政治实体\",\n",
    "        \"ORG.NAM\": \"组织\",\n",
    "        \"ORG.NOM\": \"组织的象征\",\n",
    "        \"LOC.NOM\": \"地点的象征\",\n",
    "        \"GPE.NOM\": \"政治实体的象征\",\n",
    "        \"ORG\": \"组织\",\n",
    "        \"LOC\": \"地点\",\n",
    "        \"PER\": \"人\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"测量单位\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer():\n",
    "    a = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 适用于`CONLL`的预训练\n",
    "loader: cnx_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.loaders import *\n",
    "import pickle\n",
    "from tqdm import *\n",
    "from CC.loaders.utils import *\n",
    "import json\n",
    "from CC.pre_trained import NERPreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/bert_config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': './data/weibonew/train.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': './data/weibo/pretrained_labels.txt',\n",
    "    'loader_name': 'cnx_loader',\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'use_json': True,\n",
    "    'model_name': 'Bert',\n",
    "    'task_name': 'Weibo_CNX_Pretrained',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"PER.NOM\": \"人的象征\",\n",
    "        \"LOC.NAM\": \"地点\",\n",
    "        \"PER.NAM\": \"人\",\n",
    "        \"GPE.NAM\": \"政治实体\",\n",
    "        \"ORG.NAM\": \"组织\",\n",
    "        \"ORG.NOM\": \"组织的象征\",\n",
    "        \"LOC.NOM\": \"地点的象征\",\n",
    "        \"GPE.NOM\": \"政治实体的象征\",\n",
    "        \"ORG\": \"组织\",\n",
    "        \"LOC\": \"地点\",\n",
    "        \"PER\": \"人\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"测量单位\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "pre_trainer = NERPreTrainer(**args)\n",
    "\n",
    "for i in pre_trainer():\n",
    "    a = i"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e1e097b6c3c5a2a39328ddbc7de6327b7bd71c15618bc750f041eecacee4167"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pcpower': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
