{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小样本实验\n",
    "数据规模：0.25k 0.5k 1k 1.35k  \n",
    "抽样方式：随机抽样，设置统一的样本种子   \n",
    "结果：每个模型训练3次，取最好的一次作为结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weibo小样本数据实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据样本\n",
    "import os\n",
    "\n",
    "path = \"data/few_shot\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制微博数据\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "data = [\"train.json\",\"dev.json\",\"test.json\",\"labels.txt\"]\n",
    "\n",
    "if not os.path.exists(\"data/few_shot/weibo\"):\n",
    "    os.makedirs(\"data/few_shot/weibo\")\n",
    "for f in data:\n",
    "    copyfile(f\"data/weibo/{f}\",f\"data/few_shot/weibo/{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽样数据\n",
    "postfix = [250,500,1000,1350]\n",
    "\n",
    "# 设置统一的随机种子\n",
    "import random\n",
    "\n",
    "# 读取原始训练集\n",
    "train_file = \"data/few_shot/weibo/train.json\"\n",
    "train_data = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "with open(train_file,\"r\",encoding=\"utf-8\") as reader:\n",
    "    for line in tqdm(reader,desc=f\"load {train_file}\"):\n",
    "        data = json.loads(line.strip())\n",
    "        text,label = data[\"text\"],data[\"label\"]\n",
    "        assert len(text)==len(label)\n",
    "        if len(text)>0:\n",
    "            train_data.append(data)\n",
    "        else:\n",
    "            print(text,label)\n",
    "\n",
    "# 生成数据\n",
    "for post in postfix:\n",
    "    # 重置随机，确保结果可复现\n",
    "    random.seed(2021)\n",
    "    data = random.sample(train_data,post)\n",
    "    assert len(data)==post\n",
    "    with open(f\"data/few_shot/weibo/train_{post}.json\",\"w\",encoding=\"utf-8\") as f: \n",
    "        for line in tqdm(data,desc=f\"{post}\"):\n",
    "            f.write(f\"{json.dumps(line,ensure_ascii=False)}\\n\")\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用BertBase 作为 baseline\n",
    "\n",
    "# TODO: 将Bert与CRF直接结合在一起，直接使用Transformers保存预训练\n",
    "# 方法一 Bert+CRF\n",
    "# loader: cnloader\n",
    "# model: Bert\n",
    "# data: 250x weibo.train.json\n",
    "# name: x1 x2 x3\n",
    "model_args = {\n",
    "    \"model_name\":\"Bert\",\n",
    "    \"loader\":\"cn_loader\",\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'hidden_dim': 512,\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/few_shot/weibo/train_250.json',\n",
    "    'eval_file': 'data/few_shot/weibo/dev.json',\n",
    "    'test_file': 'data/few_shot/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/few_shot/weibo/labels.txt',\n",
    "    'loader_name': 'cn_loader',\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'use_json': True,\n",
    "    'model_name': 'Bert',\n",
    "    'output_eval': True,\n",
    "    'task_name': 'weibo_bert_crf_250_x2',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "}\n",
    "\n",
    "from CC.trainer import NERTrainer\n",
    "\n",
    "trainer = NERTrainer(**model_args)\n",
    "\n",
    "for _ in trainer.train():\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: weibo.train.json.500x\n",
    "# name: x1 x2 x3\n",
    "model_args['train_file'] = \"data/few_shot/weibo/train_500.json\"\n",
    "model_args['task_name'] = 'weibo_bert_crf_500_x1'\n",
    "\n",
    "trainer = NERTrainer(**model_args)\n",
    "\n",
    "for _ in trainer.train():\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9627571863819468058fb8a0d45e3ad069ccb5b5ca291368ca8fe24c04521c7e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ccner': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
