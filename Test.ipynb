{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小样本实验\n",
    "数据规模：0.25k 0.5k 1k 1.35k  \n",
    "抽样方式：随机抽样，设置统一的样本种子   \n",
    "结果：每个模型训练3次，取最好的一次作为结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据样本\n",
    "import os\n",
    "\n",
    "path = \"data/few_shot\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 微博数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制微博数据\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "data = [\"train.json\",\"dev.json\",\"test.json\",\"labels.txt\"]\n",
    "\n",
    "if not os.path.exists(\"data/few_shot/weibo\"):\n",
    "    os.makedirs(\"data/few_shot/weibo\")\n",
    "for f in data:\n",
    "    copyfile(f\"data/weibo/{f}\",f\"data/few_shot/weibo/{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽样数据\n",
    "postfix = [250,500,1000,1350]\n",
    "\n",
    "# 设置统一的随机种子\n",
    "import random\n",
    "\n",
    "# 读取原始训练集\n",
    "train_file = \"data/few_shot/weibo/train.json\"\n",
    "train_data = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "with open(train_file,\"r\",encoding=\"utf-8\") as reader:\n",
    "    for line in tqdm(reader,desc=f\"load {train_file}\"):\n",
    "        data = json.loads(line.strip())\n",
    "        text,label = data[\"text\"],data[\"label\"]\n",
    "        assert len(text)==len(label)\n",
    "        if len(text)>0:\n",
    "            train_data.append(data)\n",
    "        else:\n",
    "            print(text,label)\n",
    "\n",
    "# 生成数据\n",
    "for post in postfix:\n",
    "    # 重置随机，确保结果可复现\n",
    "    random.seed(2021)\n",
    "    data = random.sample(train_data,post)\n",
    "    assert len(data)==post\n",
    "    with open(f\"data/few_shot/weibo/train_{post}.json\",\"w\",encoding=\"utf-8\") as f: \n",
    "        for line in tqdm(data,desc=f\"{post}\"):\n",
    "            f.write(f\"{json.dumps(line,ensure_ascii=False)}\\n\")\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notonotes4 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制Ontonotes4数据\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "data = [\"train.json\",\"dev.json\",\"test.json\",\"labels.txt\"]\n",
    "\n",
    "if not os.path.exists(\"data/few_shot/note4\"):\n",
    "    os.makedirs(\"data/few_shot/note4\")\n",
    "for f in data:\n",
    "    copyfile(f\"data/lebert/dataset/NER/note4/{f}\",f\"data/few_shot/note4/{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽样数据\n",
    "postfix = [250,500,1000,1350]\n",
    "\n",
    "# 设置统一的随机种子\n",
    "import random\n",
    "\n",
    "# 读取原始训练集\n",
    "train_file = \"data/few_shot/note4/train.json\"\n",
    "train_data = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "with open(train_file,\"r\",encoding=\"utf-8\") as reader:\n",
    "    for line in tqdm(reader,desc=f\"load {train_file}\"):\n",
    "        data = json.loads(line.strip())\n",
    "        text,label = data[\"text\"],data[\"label\"]\n",
    "        assert len(text)==len(label)\n",
    "        if len(text)>0:\n",
    "            train_data.append(data)\n",
    "        else:\n",
    "            print(text,label)\n",
    "\n",
    "# 生成数据\n",
    "for post in postfix:\n",
    "    # 重置随机，确保结果可复现\n",
    "    random.seed(2021)\n",
    "    data = random.sample(train_data,post)\n",
    "    assert len(data)==post\n",
    "    with open(f\"data/few_shot/note4/train_{post}.json\",\"w\",encoding=\"utf-8\") as f: \n",
    "        for line in tqdm(data,desc=f\"{post}\"):\n",
    "            f.write(f\"{json.dumps(line,ensure_ascii=False)}\\n\")\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MSRA 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制MSRA数据\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "data = [\"train.json\",\"dev.json\",\"test.json\",\"labels.txt\"]\n",
    "\n",
    "if not os.path.exists(\"data/few_shot/msra\"):\n",
    "    os.makedirs(\"data/few_shot/msra\")\n",
    "for f in data:\n",
    "    copyfile(f\"data/lebert/dataset/NER/msra/{f}\",f\"data/few_shot/msra/{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽样数据\n",
    "postfix = [250,500,1000,1350]\n",
    "\n",
    "# 设置统一的随机种子\n",
    "import random\n",
    "\n",
    "# 读取原始训练集\n",
    "train_file = \"data/few_shot/msra/train.json\"\n",
    "train_data = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "with open(train_file,\"r\",encoding=\"utf-8\") as reader:\n",
    "    for line in tqdm(reader,desc=f\"load {train_file}\"):\n",
    "        data = json.loads(line.strip())\n",
    "        text,label = data[\"text\"],data[\"label\"]\n",
    "        assert len(text)==len(label)\n",
    "        if len(text)>0:\n",
    "            train_data.append(data)\n",
    "        else:\n",
    "            print(text,label)\n",
    "\n",
    "# 生成数据\n",
    "for post in postfix:\n",
    "    # 重置随机，确保结果可复现\n",
    "    random.seed(2021)\n",
    "    data = random.sample(train_data,post)\n",
    "    assert len(data)==post\n",
    "    with open(f\"data/few_shot/msra/train_{post}.json\",\"w\",encoding=\"utf-8\") as f: \n",
    "        for line in tqdm(data,desc=f\"{post}\"):\n",
    "            f.write(f\"{json.dumps(line,ensure_ascii=False)}\\n\")\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resume数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制Resume数据\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "data = [\"train.json\",\"dev.json\",\"test.json\",\"labels.txt\"]\n",
    "\n",
    "if not os.path.exists(\"data/few_shot/resume\"):\n",
    "    os.makedirs(\"data/few_shot/resume\")\n",
    "for f in data:\n",
    "    copyfile(f\"data/lebert/dataset/NER/resume/{f}\",f\"data/few_shot/resume/{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽样数据\n",
    "postfix = [250,500,1000,1350]\n",
    "\n",
    "# 设置统一的随机种子\n",
    "import random\n",
    "\n",
    "# 读取原始训练集\n",
    "train_file = \"data/few_shot/resume/train.json\"\n",
    "train_data = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "with open(train_file,\"r\",encoding=\"utf-8\") as reader:\n",
    "    for line in tqdm(reader,desc=f\"load {train_file}\"):\n",
    "        data = json.loads(line.strip())\n",
    "        text,label = data[\"text\"],data[\"label\"]\n",
    "        assert len(text)==len(label)\n",
    "        if len(text)>0:\n",
    "            train_data.append(data)\n",
    "        else:\n",
    "            print(text,label)\n",
    "\n",
    "# 生成数据\n",
    "for post in postfix:\n",
    "    # 重置随机，确保结果可复现\n",
    "    random.seed(2021)\n",
    "    data = random.sample(train_data,post)\n",
    "    assert len(data)==post\n",
    "    with open(f\"data/few_shot/resume/train_{post}.json\",\"w\",encoding=\"utf-8\") as f: \n",
    "        for line in tqdm(data,desc=f\"{post}\"):\n",
    "            f.write(f\"{json.dumps(line,ensure_ascii=False)}\\n\")\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert+LSTM+CRF(Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    \"model_name\":\"Bert\",\n",
    "    \"loader\":\"cn_loader\",\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'hidden_dim': 512,\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/few_shot/weibo/train_250.json',\n",
    "    'eval_file': 'data/few_shot/weibo/dev.json',\n",
    "    'test_file': 'data/few_shot/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/few_shot/weibo/labels.txt',\n",
    "    'loader_name': 'cn_loader',\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'use_json': True,\n",
    "    'model_name': 'Bert',\n",
    "    'output_eval': True,\n",
    "    'task_name': 'weibo_bert_crf_250_x3',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "import torch\n",
    "\n",
    "per_task_count = 3\n",
    "\n",
    "datasets = [\"weibo\",\"note4\",\"msra\",\"resume\"]\n",
    "\n",
    "task_trainset = [250,500,1000,1350]\n",
    "\n",
    "tasks_args = {\n",
    "        \"train_file\":\"data/few_shot/{}/train_{}.json\",\n",
    "        \"eval_file\":\"data/few_shot/{}/dev.json\",\n",
    "        \"test_file\":\"data/few_shot/{}/test.json\",\n",
    "        'tag_file':\"data/few_shot/{}/labels.txt\",\n",
    "        \"task_name\":\"{}_bert_crf_{}_{}\",\n",
    "}\n",
    "\n",
    "for name in datasets:\n",
    "    for trainset in task_trainset:\n",
    "        for i in range(per_task_count):\n",
    "            for key,value in tasks_args.items():\n",
    "                model_args[key] = value.format(name,trainset,f\"x{i+1}\")\n",
    "            trainer = NERTrainer(**model_args)\n",
    "\n",
    "            for _ in trainer.train():\n",
    "                pass\n",
    "\n",
    "            torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEBert+LSTM+CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    \"loader\":\"le_loader\",\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'hidden_dim': 512,\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/few_shot/weibo/train_250.json',\n",
    "    'eval_file': 'data/few_shot/weibo/dev.json',\n",
    "    'test_file': 'data/few_shot/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'word_embedding_file': 'data/tencent/word_embedding.txt',\n",
    "    'tag_file': 'data/few_shot/weibo/labels.txt',\n",
    "    'word_vocab_file':'data/tencent/tencent_vocab.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'use_json': True,\n",
    "    'model_name': 'LEBert',\n",
    "    'output_eval': True,\n",
    "    'task_name': 'weibo_lebert_crf_250_x3',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "import torch\n",
    "\n",
    "per_task_count = 3\n",
    "\n",
    "datasets = [\"weibo\",\"note4\",\"msra\",\"resume\"]\n",
    "\n",
    "task_trainset = [250,500,1000,1350]\n",
    "\n",
    "tasks_args = {\n",
    "        \"train_file\":\"data/few_shot/{}/train_{}.json\",\n",
    "        \"eval_file\":\"data/few_shot/{}/dev.json\",\n",
    "        \"test_file\":\"data/few_shot/{}/test.json\",\n",
    "        'tag_file':\"data/few_shot/{}/labels.txt\",\n",
    "        \"task_name\":\"{}_lebert_crf_{}_{}\"\n",
    "}\n",
    "\n",
    "for name in datasets:\n",
    "    for trainset in task_trainset:\n",
    "        for i in range(per_task_count):\n",
    "            for key,value in tasks_args.items():\n",
    "                model_args[key] = value.format(name,trainset,f\"x{i+1}\")\n",
    "            trainer = NERTrainer(**model_args)\n",
    "\n",
    "            for _ in trainer.train():\n",
    "                pass\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompts PreTrain+LeBert FineTune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PreTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weibo\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "import torch\n",
    "\n",
    "pretrain_model_args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/few_shot/weibo/train_250.json',\n",
    "    'eval_file': 'data/few_shot/weibo/dev.json',\n",
    "    'test_file': 'data/few_shot/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/few_shot/weibo/labels.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'weibo_pretrain_lebert_250_pretraind_task',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\":\"非实体\",\n",
    "        \"PER.NOM\": \"指代人名\",\n",
    "        \"LOC.NAM\": \"地名\",\n",
    "        \"PER.NAM\": \"人名\",\n",
    "        \"GPE.NAM\": \"政体\",\n",
    "        \"ORG.NAM\": \"机构\",\n",
    "        \"ORG.NOM\": \"指代机构\",\n",
    "        \"LOC.NOM\": \"指代地名\",\n",
    "        \"GPE.NOM\": \"指代政体\",\n",
    "        \"ORG\": \"组织\",\n",
    "        \"LOC\": \"地点\",\n",
    "        \"PER\": \"人\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"测量单位\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "trainsets = [250,500,1000,1350]\n",
    "\n",
    "for train in trainsets:\n",
    "    pretrain_model_args['train_file'] = f'data/few_shot/weibo/train_{train}.json'\n",
    "    pretrain_model_args['task_name'] = f'weibo_pretrain_lebert_{train}_pretraind_task'\n",
    "\n",
    "    pre_trainer = NERPreTrainer(**pretrain_model_args)\n",
    "\n",
    "    for i in pre_trainer():\n",
    "        a = i\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note4\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "import torch\n",
    "\n",
    "pretrain_model_args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/few_shot/note4/train_250.json',\n",
    "    'eval_file': 'data/few_shot/note4/dev.json',\n",
    "    'test_file': 'data/few_shot/note4/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/few_shot/note4/labels.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'note4_pretrain_lebert_250_pretraind_task',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\":\"非实体\",\n",
    "        \"ORG\": \"组织\",\n",
    "        \"LOC\": \"地点\",\n",
    "        \"PER\": \"人\",\n",
    "        \"GPE\": \"政治\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"测量单位\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "trainsets = [250,500,1000,1350]\n",
    "\n",
    "for train in trainsets:\n",
    "    pretrain_model_args['train_file'] = f'data/few_shot/note4/train_{train}.json'\n",
    "    pretrain_model_args['task_name'] = f'note4_pretrain_lebert_{train}_pretraind_task'\n",
    "\n",
    "    pre_trainer = NERPreTrainer(**pretrain_model_args)\n",
    "\n",
    "    for i in pre_trainer():\n",
    "        a = i\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msra\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "import torch\n",
    "\n",
    "pretrain_model_args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/few_shot/msra/train_250.json',\n",
    "    'eval_file': 'data/few_shot/msra/dev.json',\n",
    "    'test_file': 'data/few_shot/msra/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/few_shot/msra/labels.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'msra_pretrain_lebert_250_pretraind_task',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\":\"非实体\",\n",
    "        \"NS\":\"地名\",\n",
    "        \"NR\":\"人名\",\n",
    "        \"NT\":\"机构团体\",\n",
    "        \"ORG\": \"组织\",\n",
    "        \"LOC\": \"地点\",\n",
    "        \"PER\": \"人\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"测量单位\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "trainsets = [250,500,1000,1350]\n",
    "\n",
    "for train in trainsets:\n",
    "    pretrain_model_args['train_file'] = f'data/few_shot/msra/train_{train}.json'\n",
    "    pretrain_model_args['task_name'] = f'msra_pretrain_lebert_{train}_pretraind_task'\n",
    "\n",
    "    pre_trainer = NERPreTrainer(**pretrain_model_args)\n",
    "\n",
    "    for i in pre_trainer():\n",
    "        a = i\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume\n",
    "from CC.pre_trained import NERPreTrainer\n",
    "import torch\n",
    "\n",
    "pretrain_model_args = {\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 512,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/few_shot/resume/train_250.json',\n",
    "    'eval_file': 'data/few_shot/resume/dev.json',\n",
    "    'test_file': 'data/few_shot/resume/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'tag_file': 'data/few_shot/resume/labels.txt',\n",
    "    'loader_name': 'lex_loader',\n",
    "    \"word_embedding_file\": \"./data/tencent/word_embedding.txt\",\n",
    "    \"word_vocab_file\": \"./data/tencent/tencent_vocab.txt\",\n",
    "    \"word_vocab_file_with_tag\": \"./data/tencent/tencent_vocab_with_tag.json\",\n",
    "    \"default_tag\": \"O\",\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'task_name': 'resume_pretrain_lebert_250_pretraind_task',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "    \"tag_rules\": {\n",
    "        \"O\":\"非实体\",\n",
    "        \"NAME\":\"名字\",\n",
    "        \"CONT\":\"国家\",\n",
    "        \"RACE\":\"种族背景\",\n",
    "        \"TITLE\":\"职位\",\n",
    "        \"EDU\":\"教育机构\",\n",
    "        \"PRO\":\"专业\",\n",
    "        \"ORG\": \"组织\",\n",
    "        \"LOC\": \"地点\",\n",
    "        \"PER\": \"人\",\n",
    "        \"Time\": \"时间\",\n",
    "        \"Thing\": \"物品\",\n",
    "        \"Metric\": \"测量单位\",\n",
    "        \"Abstract\": \"作品\",\n",
    "        \"Physical\": \"实体\",\n",
    "        \"Term\": \"术语\",\n",
    "        \"company\": \"企业\",\n",
    "        \"name\": \"名字\",\n",
    "        \"game\": \"游戏\",\n",
    "        \"movie\": \"电影\",\n",
    "        \"position\": \"职位\",\n",
    "        \"address\": \"地址\",\n",
    "        \"government\": \"政府\",\n",
    "        \"scene\": \"景点\",\n",
    "        \"book\": \"书名\"\n",
    "    }\n",
    "}\n",
    "\n",
    "trainsets = [250,500,1000,1350]\n",
    "\n",
    "for train in trainsets:\n",
    "    pretrain_model_args['train_file'] = f'data/few_shot/resume/train_{train}.json'\n",
    "    pretrain_model_args['task_name'] = f'resume_pretrain_lebert_{train}_pretraind_task'\n",
    "\n",
    "    pre_trainer = NERPreTrainer(**pretrain_model_args)\n",
    "\n",
    "    for i in pre_trainer():\n",
    "        a = i\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    \"loader\":\"le_loader\",\n",
    "    'num_epochs': 30,\n",
    "    'num_gpus': [0, 1, 2, 3],\n",
    "    'hidden_dim': 512,\n",
    "    'bert_config_file_name': './model/chinese_wwm_ext/config.json',\n",
    "    'pretrained_file_name': './model/chinese_wwm_ext/pytorch_model.bin',\n",
    "    'max_seq_length': 150,\n",
    "    'max_scan_num': 1000000,\n",
    "    'train_file': 'data/few_shot/weibo/train_250.json',\n",
    "    'eval_file': 'data/few_shot/weibo/dev.json',\n",
    "    'test_file': 'data/few_shot/weibo/test.json',\n",
    "    'bert_vocab_file': './model/chinese_wwm_ext/vocab.txt',\n",
    "    'word_embedding_file': 'data/tencent/word_embedding.txt',\n",
    "    'tag_file': 'data/few_shot/weibo/labels.txt',\n",
    "    'word_vocab_file':'data/tencent/tencent_vocab.txt',\n",
    "    'loader_name': 'le_loader',\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 64,\n",
    "    'do_shuffle': True,\n",
    "    'use_json': True,\n",
    "    'model_name': 'LEBert',\n",
    "    'output_eval': True,\n",
    "    'task_name': 'weibo_lebert_crf_250_x3',\n",
    "    \"use_gpu\": True,\n",
    "    \"debug\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CC.trainer import NERTrainer\n",
    "import torch\n",
    "\n",
    "per_task_count = 3\n",
    "\n",
    "datasets = [\"weibo\",\"note4\",\"msra\",\"resume\"]\n",
    "\n",
    "task_trainset = [250,500,1000,1350]\n",
    "\n",
    "pretrain = [240,480,960,1290]\n",
    "\n",
    "# dataset,counts,time,others\n",
    "tasks_args = {\n",
    "        'pretrained_file_name': 'save_pretrained/{0}_pretrain_lebert_{1}_pretraind_task/Bert_{3}/pytorch_model.bin',\n",
    "        \"train_file\":\"data/few_shot/{0}/train_{1}.json\",\n",
    "        \"eval_file\":\"data/few_shot/{0}/dev.json\",\n",
    "        \"test_file\":\"data/few_shot/{0}/test.json\",\n",
    "        'tag_file':\"data/few_shot/{0}/labels.txt\",\n",
    "        \"task_name\":\"{0}_pretrain_lebert_crf_{1}_{2}\"\n",
    "        \n",
    "}\n",
    "\n",
    "for name in datasets:\n",
    "    for trainset,pre in zip(task_trainset,pretrain): \n",
    "        for i in range(per_task_count):\n",
    "            for key,value in tasks_args.items():\n",
    "                model_args[key] = value.format(name,trainset,f\"x{i+1}\",pre)\n",
    "            trainer = NERTrainer(**model_args)\n",
    "\n",
    "            for _ in trainer.train():\n",
    "                pass\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9627571863819468058fb8a0d45e3ad069ccb5b5ca291368ca8fe24c04521c7e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ccner': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
